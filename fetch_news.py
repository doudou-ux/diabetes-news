# fetch_news.py
import datetime
import html
import os
import time
import requests
import feedparser
from bs4 import BeautifulSoup

# --- (1) é…ç½®æƒå¨ RSS æº ---
# è¯·æ‚¨å°†æ¥æ›¿æ¢ YOUR_PUBMED_RSS_URL å’Œ YOUR_ADA_JOURNAL_RSS_URL ä¸ºå®é™…çš„é“¾æ¥
# æ‚¨å¯ä»¥æ·»åŠ æ›´å¤šç±»ä¼¼çš„æ¡ç›®
AUTHORITATIVE_RSS_FEEDS = [
    {
        "url": "https://www.medscape.com/rss/public/diabetes.xml",
        "source_override": "Medscape Diabetes", # æ˜¾ç¤ºåœ¨æ–°é—»å¡ç‰‡ä¸Šçš„æ¥æºåç§°
        "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"] # è¿™æ¡RSSæºçš„æ–°é—»ä¼šè¢«å°è¯•æ”¾å…¥è¿™äº›åˆ†ç±»
    },
    {
        "url": "https://www.healio.com/news/endocrinology/rss",
        "source_override": "Healio Endocrinology",
        "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"]
    },
    # --- è¯·æ‚¨æ›¿æ¢æˆ–æ·»åŠ ä»¥ä¸‹å ä½ç¬¦ ---
    # {
    #     "url": "YOUR_PUBMED_DIABETES_RESEARCH_RSS_URL", # ä¾‹å¦‚: https://pubmed.ncbi.nlm.nih.gov/rss/search/...
    #     "source_override": "PubMed",
    #     "target_categories": ["æœ€æ–°ç ”ç©¶"]
    # },
    # {
    #     "url": "YOUR_ADA_DIABETES_CARE_RSS_URL", # ä¾‹å¦‚: https://diabetesjournals.org/care/rss/current.xml
    #     "source_override": "Diabetes Care (ADA)",
    #     "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"]
    # },
]

# --- (2) é…ç½®ç½‘ç«™å±•ç¤ºçš„åˆ†ç±»åŠå¯¹åº”çš„ Google News è¡¥å……å…³é”®è¯ ---
CATEGORIES_CONFIG = {
    "æœ€æ–°ç ”ç©¶": {
        "keywords": "ç³–å°¿ç—… æœ€æ–°è®ºæ–‡ OR ç³–å°¿ç—…æŠ€æœ¯çªç ´ OR ç³–å°¿ç—…æœºåˆ¶ç ”ç©¶ OR åŒ»å­¦ä¼šè®®ç³–å°¿ç—… OR GLP-1ç³–å°¿ç—… OR SGLT2ç³–å°¿ç—… OR èƒ°å²›Î²ç»†èƒ OR èƒ°å²›ç´ æ•æ„Ÿæ€§",
        "emoji": "ğŸ”¬"
    },
    "æ²»ç–—è¿›å±•": {
        "keywords": "ç³–å°¿ç—…æ–°è¯ OR ç³–å°¿ç—…é€‚åº”ç—‡æ‰©å±• OR ç³–å°¿ç—…è®¾å¤‡ç ”å‘ OR AIè¾…åŠ©è¯Šç–—ç³–å°¿ç—… OR è¾¾æ ¼åˆ—å‡€ OR å¸ç¾æ ¼é²è‚½ OR CGM OR è¿ç»­è¡€ç³–ç›‘æµ‹ OR èƒ°å²›ç´ æ³µ",
        "emoji": "ğŸ’Š"
    },
    "é¥®é£Ÿä¸è¥å…»": {
        "keywords": "ç³–å°¿ç—…é¥®é£ŸæŒ‡å— OR ç¢³æ°´äº¤æ¢è¡¨ OR ç³–å°¿ç—…é£Ÿè°± OR ä½GIé¥®é£Ÿç³–å°¿ç—… OR é«˜è›‹ç™½é¥®é£Ÿç³–å°¿ç—… OR é—´æ­‡æ€§æ–­é£Ÿç³–å°¿ç—… OR è†³é£Ÿçº¤ç»´ç³–å°¿ç—…",
        "emoji": "ğŸ¥—"
    },
    "é¢„é˜²ä¸ç”Ÿæ´»æ–¹å¼": {
        "keywords": "ç³–å°¿ç—…è¿åŠ¨å»ºè®® OR ç³–å°¿ç—…ç¡çœ  OR ç³–å°¿ç—…å‡é‡ OR æ§ç³–è®¡åˆ’ OR ç³–å°¿ç—…æ—©æœŸç­›æŸ¥ OR ç³–è€é‡å¼‚å¸¸ OR ä½“è„‚ç®¡ç†ç³–å°¿ç—… OR ç³–å°¿ç—…æ­¥æ•°ç›®æ ‡",
        "emoji": "ğŸƒâ€â™€ï¸"
    },
    "å¹¶å‘ç—‡ç®¡ç†": {
        "keywords": "ç³–å°¿ç—…è¶³ OR ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ OR ç³–å°¿ç—…è‚¾ç—… OR ç³–å°¿ç—…ç¥ç»ç—…å˜ OR ç³–ç½‘ç—… OR å¾®è¡€ç®¡ç—…å˜ç³–å°¿ç—… OR å°¿ç™½è›‹ç™½ç³–å°¿ç—…",
        "emoji": "ğŸ©º"
    },
    "æ‚£è€…æ•…äº‹ä¸å¿ƒç†æ”¯æŒ": {
        "keywords": "ç³–å°¿ç—…æ§ç³–ç»éªŒ OR ç³–å°¿ç—…å¿ƒç†æ”¯æŒ OR ç³–å°¿ç—…å®¶åº­æ”¯æŒ OR ç³–å°¿ç—…æ‚£è€…æ•…äº‹ OR ç³–å°¿ç—…åŒ»ç”Ÿé—®ç­”",
        "emoji": "ğŸ˜Š"
    },
    "æ”¿ç­–/åŒ»ä¿ä¿¡æ¯": {
        "keywords": "ç³–å°¿ç—…è¯å“çº³ä¿ OR ç³–å°¿ç—…åŒ»ä¿æŠ¥é”€ OR ç³–å°¿ç—…ç¤¾åŒºæ…¢ç—…éšè®¿ OR å›½å®¶è¯ç›‘å±€ç³–å°¿ç—…æ”¿ç­– OR åŒ»ä¿å±€ç³–å°¿ç—…æ”¿ç­–",
        "emoji": "ğŸ“„"
    }
}

# --- å¸®åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘ä¸€ä¸ªæœˆå†… ---
def is_within_last_month_rss(time_struct, today_date_obj):
    if not time_struct:
        return False
    try:
        article_date = datetime.date(time_struct.tm_year, time_struct.tm_mon, time_struct.tm_mday)
        thirty_days_ago = today_date_obj - datetime.timedelta(days=30)
        return thirty_days_ago <= article_date <= today_date_obj
    except Exception as e:
        print(f"      [is_within_last_month_rss] æ—¥æœŸè½¬æ¢é”™è¯¯: {e} - Time Struct: {time_struct}")
        return False

# --- å¸®åŠ©å‡½æ•°ï¼šæ¸…ç† HTML ---
def clean_html(raw_html):
    if not raw_html: return ""
    try:
        return BeautifulSoup(raw_html, "html.parser").get_text()
    except Exception: return raw_html

# --- (3) é€šç”¨çš„ä»å•ä¸ª RSS URL è·å–æ–‡ç« çš„å‡½æ•° ---
def fetch_articles_from_rss(rss_url, source_name_override=None):
    """
    ä»ç»™å®šçš„ RSS URL è·å–æ–‡ç« åˆ—è¡¨ã€‚
    è¿”å›è§£æåçš„æ–‡ç« åˆ—è¡¨ï¼Œæ¯ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªåŒ…å« title, link, snippet, source, time_struct çš„å­—å…¸ã€‚
    """
    print(f"    æ­£åœ¨ä»æºè·å–: {rss_url} ({source_name_override or 'æœªçŸ¥æº'})")
    articles = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(rss_url, headers=headers, timeout=20)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        if not feed.entries:
            print(f"      æ­¤æºæœªè¿”å›ä»»ä½•æ¡ç›®: {rss_url}")
            return []

        print(f"      ä»æ­¤æºåŸå§‹è·å–åˆ° {len(feed.entries)} æ¡æ–°é—»ã€‚")
        for entry in feed.entries:
            title = entry.get("title", "æ— æ ‡é¢˜")
            link = entry.get("link", f"javascript:void(0);_{title}") # æ·»åŠ æ ‡é¢˜ç¡®ä¿é“¾æ¥å”¯ä¸€æ€§ï¼ˆå¦‚æœlinkä¸ºç©ºï¼‰
            
            published_time_struct = entry.get("published_parsed") or entry.get("updated_parsed")
            
            summary_html = entry.get("summary", entry.get("description", "æš‚æ— æ‘˜è¦")) # æœ‰äº›æºç”¨description
            snippet = clean_html(summary_html)
            
            # æ¥æºå¤„ç†ï¼šä¼˜å…ˆä½¿ç”¨ overrideï¼Œå…¶æ¬¡æ˜¯ feedparser æä¾›çš„ï¼Œæœ€åæ˜¯ Google News (å¦‚æœæ˜¯ Google News æº)
            actual_source_name = source_name_override
            if not actual_source_name:
                source_info = entry.get("source")
                actual_source_name = source_info.get("title") if source_info else "æœªçŸ¥æ¥æº"
                if "news.google.com" in rss_url and not source_name_override : # ç‰¹æ®Šå¤„ç†Google Newsçš„æ¥æº
                    # Google News RSSçš„æ¡ç›®titleé€šå¸¸æ˜¯ "æ–°é—»æ ‡é¢˜ - æ¥æº"ï¼Œå°è¯•æå–
                    if ' - ' in title:
                        parts = title.rsplit(' - ', 1)
                        # title = parts[0] # å–æ¶ˆä¿®æ”¹æ ‡é¢˜ï¼Œè®©ç”¨æˆ·çœ‹åˆ°å®Œæ•´ä¿¡æ¯
                        actual_source_name = parts[1]


            articles.append({
                "title": title,
                "url": link,
                "snippet": snippet,
                "source": actual_source_name,
                "time_struct": published_time_struct # ä¿ç•™ time_struct ç”¨äºåç»­æ—¥æœŸè¿‡æ»¤
            })
    except requests.exceptions.Timeout:
        print(f"      è·å–æºæ—¶å‘ç”Ÿè¶…æ—¶é”™è¯¯: {rss_url}")
    except requests.exceptions.RequestException as e:
        print(f"      è·å–æºæ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e} (URL: {rss_url})")
    except Exception as e:
        print(f"      å¤„ç†æºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e} (URL: {rss_url})")
    return articles

# --- HTML ç”Ÿæˆé€»è¾‘ (ä¸ä¹‹å‰ç‰ˆæœ¬åŸºæœ¬ä¸€è‡´) ---
def generate_html_content(all_news_data):
    current_time_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
    app_timezone = os.getenv('APP_TIMEZONE', 'UTC') 
    if app_timezone != 'UTC':
        try:
             current_time_str = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8))).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S %Z')
        except Exception as e:
            print(f"åº”ç”¨æ—¶åŒº ({app_timezone}) æ—¶å‡ºé”™: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤æœåŠ¡å™¨æ—¶é—´ã€‚")
            current_time_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (æœåŠ¡å™¨æ—¶é—´)')

    current_year = datetime.datetime.now().year
    github_repo_url = f"https://github.com/{os.getenv('GITHUB_REPOSITORY', 'doudou-ux/diabetes-news')}"

    html_output = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç³–å°¿ç—…å‰æ²¿èµ„è®¯</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {{ font-family: 'Inter', sans-serif; background-color: #f0f4f8; }}
        .news-card {{ transition: transform 0.3s ease, box-shadow 0.3s ease; display: flex; flex-direction: column; justify-content: space-between; height: 100%; background-color: #ffffff; border-radius: 0.5rem; }}
        .news-card:hover {{ transform: translateY(-5px); box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); }}
        .category-section {{ background-color: #ffffff; border-radius: 0.75rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); padding: 1.25rem; }}
        @media (min-width: 768px) {{ .category-section {{ padding: 2rem; }} }}
        .category-title-text {{ border-bottom: 3px solid #3b82f6; padding-bottom: 0.75rem; margin-bottom: 1.75rem; color: #1e40af; font-size: 1.5rem; }}
        @media (min-width: 768px) {{ .category-title-text {{ font-size: 1.875rem; }} }}
        .news-item-link {{ color: #2563eb; text-decoration: none; font-weight: 600; }}
        .news-item-link:hover {{ text-decoration: underline; color: #1d4ed8; }}
        .source-tag, .time-tag {{ padding: 0.25rem 0.75rem; border-radius: 0.375rem; font-size: 0.75rem; font-weight: 500; line-height: 1.25; }}
        .source-tag {{ background-color: #e0e7ff; color: #3730a3; }}
        .time-tag {{ background-color: #d1fae5; color: #065f46; }}
        .card-content {{ flex-grow: 1; }}
        .card-footer {{ margin-top: auto; }}
        .header-main-title {{ font-size: 1.875rem; }}
        @media (min-width: 640px) {{ .header-main-title {{ font-size: 2.25rem; }} }}
        @media (min-width: 768px) {{ .header-main-title {{ font-size: 3rem; }} }}
        .main-container {{ padding: 1rem; }}
        @media (min-width: 768px) {{ .main-container {{ padding: 2rem; }} }}
        .loader {{ border: 5px solid #f3f3f3; border-radius: 50%; border-top: 5px solid #3498db; width: 50px; height: 50px; -webkit-animation: spin 1s linear infinite; animation: spin 1s linear infinite; margin: 20px auto; }}
        @-webkit-keyframes spin {{ 0% {{ -webkit-transform: rotate(0deg); }} 100% {{ -webkit-transform: rotate(360deg); }} }}
        @keyframes spin {{ 0% {{ transform: rotate(0deg); }} 100% {{ transform: rotate(360deg); }} }}
    </style>
</head>
<body class="bg-gray-100 text-gray-800">
    <div class="container mx-auto main-container">
        <header class="text-center mb-10 md:mb-16">
            <h1 class="font-bold text-blue-700 header-main-title">ç³–å°¿ç—…å‰æ²¿èµ„è®¯</h1>
            <p class="text-gray-600 mt-3 text-base md:text-lg">æœ€è¿‘ä¸€ä¸ªæœˆåŠ¨æ€ï¼ˆè‡ªåŠ¨æ›´æ–°äºï¼š<span id="updateTime">{current_time_str}</span>ï¼‰</p>
            <p class="text-sm text-gray-500 mt-2">èµ„è®¯ç»¼åˆæ¥æº</p>
        </header>
        <div id="news-content" class="space-y-12">
            <div id="loading-indicator" class="text-center py-10">
                 <div class="loader"></div>
                 <p class="text-gray-600 mt-2">æ­£åœ¨åŠ è½½æœ€æ–°èµ„è®¯...</p>
            </div>
        </div>
    </div>
    <footer class="text-center p-6 mt-12 text-gray-600 text-sm border-t border-gray-300">
        <p>&copy; {current_year} ç³–å°¿ç—…èµ„è®¯èšåˆ. <a href="{github_repo_url}" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">é¡¹ç›®æºç </a></p>
        <p class="mt-1">æœ¬ç«™å†…å®¹ä»…ä¾›å‚è€ƒ, ä¸æ„æˆåŒ»ç–—å»ºè®®ã€‚</p>
    </footer>
    <script>
        document.addEventListener('DOMContentLoaded', function() {{
            const newsContent = document.getElementById('news-content');
            const loadingIndicator = document.getElementById('loading-indicator');
            let hasRealContent = false;
            if (newsContent && newsContent.childNodes) {{
                for (let i = 0; i < newsContent.childNodes.length; i++) {{
                    if (newsContent.childNodes[i].id !== 'loading-indicator' && newsContent.childNodes[i].nodeType === 1) {{
                        hasRealContent = true;
                        break;
                    }}
                }}
            }}
            if (loadingIndicator && hasRealContent) {{
                // loadingIndicator.style.display = 'none'; 
            }} else if (loadingIndicator && newsContent && newsContent.textContent && newsContent.textContent.includes("æœªèƒ½åŠ è½½")) {{
                loadingIndicator.style.display = 'none';
            }}
        }});
    </script>
</body>
</html>"""

    news_html_parts = []
    found_any_news = False

    if not all_news_data or all(not articles for articles in all_news_data.values()):
        news_html_parts.append('<p class="text-center text-gray-500 text-xl py-10">æŠ±æ­‰ï¼Œç›®å‰æœªèƒ½åŠ è½½åˆ°æœ€è¿‘ä¸€ä¸ªæœˆç›¸å…³çš„ç³–å°¿ç—…èµ„è®¯ã€‚</p>')
    else:
        for category, articles in all_news_data.items():
            category_emoji = CATEGORIES_CONFIG.get(category, {}).get("emoji", "")
            category_html = f"""
            <section class="category-section">
                <h2 class="font-semibold category-title-text">{category_emoji} {html.escape(category)}</h2>
            """
            if not articles:
                category_html += '<p class="text-gray-500">æœ€è¿‘ä¸€ä¸ªæœˆæš‚æ— è¯¥åˆ†ç±»ä¸‹çš„èµ„è®¯ã€‚</p>'
            else:
                found_any_news = True
                category_html += '<div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6 md:gap-8 news-grid">'
                for article in articles: 
                    title = html.escape(article.get('title', 'æ— æ ‡é¢˜'))
                    url = html.escape(article.get('url', 'javascript:void(0);'))
                    snippet_raw = article.get('snippet', 'æš‚æ— æ‘˜è¦')
                    snippet = html.escape(snippet_raw[:150] + ('...' if len(snippet_raw) > 150 else ''))
                    source_display = html.escape(article.get('source', 'æœªçŸ¥æ¥æº')) # ä½¿ç”¨ article ä¸­å·²å¤„ç†å¥½çš„ source
                    time_display = html.escape(article.get('time_display_str', 'æœªçŸ¥æ—¶é—´')) # ä½¿ç”¨ article ä¸­å·²å¤„ç†å¥½çš„ time_display_str

                    category_html += f"""
                    <div class="news-card shadow-md hover:shadow-lg p-5">
                        <div class="card-content">
                            <h3 class="text-lg mb-2">
                                <a href="{url}" {'target="_blank" rel="noopener noreferrer"' if url != 'javascript:void(0);' else ''} class="news-item-link">
                                    {title}
                                </a>
                            </h3>
                            <p class="text-gray-600 text-sm mb-4 leading-relaxed">{snippet}</p>
                        </div>
                        <div class="card-footer text-xs text-gray-500 flex flex-wrap gap-2 items-center pt-3 border-t border-gray-200">
                            <span class="source-tag">{source_display}</span>
                            <span class="time-tag">{time_display}</span>
                        </div>
                    </div>
                    """
                category_html += "</div>" 
            category_html += "</section>"
            news_html_parts.append(category_html)
    
    loading_indicator_html = """<div id="loading-indicator" class="text-center py-10">
                 <div class="loader"></div>
                 <p class="text-gray-600 mt-2">æ­£åœ¨åŠ è½½æœ€æ–°èµ„è®¯...</p>
            </div>"""
    if loading_indicator_html in html_output:
        if found_any_news or (news_html_parts and "<p class=\"text-center text-gray-500 text-xl py-10\">" in news_html_parts[0]):
            html_output = html_output.replace(loading_indicator_html, "".join(news_html_parts))
        elif not news_html_parts: 
             html_output = html_output.replace(loading_indicator_html, '<p class="text-center text-gray-500 text-xl py-10">èµ„è®¯åŠ è½½æ—¶å‡ºç°é—®é¢˜æˆ–æš‚æ— å†…å®¹ã€‚</p>')
    else: 
        html_output = html_output.replace(
            '<div id="news-content" class="space-y-12">\n            \n            ', 
            f'<div id="news-content" class="space-y-12">\n{"".join(news_html_parts)}'
        )
    return html_output

# --- (4) ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == "__main__":
    print("å¼€å§‹ä»å¤šä¸ª RSS æºç”Ÿæˆç³–å°¿ç—…èµ„è®¯ç½‘é¡µ...")
    
    # åˆå§‹åŒ–å­˜å‚¨æ‰€æœ‰åˆ†ç±»æ–‡ç« çš„å­—å…¸
    # key æ˜¯ç½‘ç«™åˆ†ç±»å, value æ˜¯è¯¥åˆ†ç±»ä¸‹çš„æ–‡ç« åˆ—è¡¨
    all_articles_by_site_category = {category_name: [] for category_name in CATEGORIES_CONFIG.keys()}
    seen_article_links = set() # ç”¨äºå»é‡
    today = datetime.date.today()
    MAX_ARTICLES_PER_CATEGORY = 10

    # --- æ­¥éª¤ä¸€ï¼šä»æƒå¨ RSS æºè·å–æ–°é—» ---
    print("\n--- æ­£åœ¨ä»æƒå¨ RSS æºè·å–æ–°é—» ---")
    for feed_info in AUTHORITATIVE_RSS_FEEDS:
        raw_articles_from_feed = fetch_articles_from_rss(feed_info["url"], feed_info["source_override"])
        print(f"  å¤„ç†æ¥è‡ª {feed_info['source_override']} çš„ {len(raw_articles_from_feed)} æ¡åŸå§‹æ–°é—»...")
        
        for article_data in raw_articles_from_feed:
            if article_data["url"] in seen_article_links:
                print(f"    è·³è¿‡é‡å¤æ–‡ç«  (æ¥è‡ªæƒå¨æº): {article_data['title'][:30]}...")
                continue

            if is_within_last_month_rss(article_data["time_struct"], today):
                time_display_str = "æœªçŸ¥æ—¶é—´"
                if article_data["time_struct"]:
                    try:
                        time_display_str = time.strftime("%Y-%m-%d", article_data["time_struct"])
                    except: pass
                
                processed_article = {
                    "title": article_data["title"],
                    "url": article_data["url"],
                    "snippet": article_data["snippet"],
                    "source": article_data["source"], # ä½¿ç”¨ fetch_articles_from_rss ä¸­å¤„ç†å¥½çš„ source
                    "time_display_str": time_display_str
                }

                seen_article_links.add(article_data["url"])
                
                # å°†æ–‡ç« æ”¾å…¥å…¶å¯¹åº”çš„æ‰€æœ‰ç›®æ ‡åˆ†ç±»
                for target_cat in feed_info["target_categories"]:
                    if target_cat in all_articles_by_site_category: # ç¡®ä¿ç›®æ ‡åˆ†ç±»æ˜¯æˆ‘ä»¬ç½‘ç«™å®šä¹‰çš„åˆ†ç±»
                        if len(all_articles_by_site_category[target_cat]) < MAX_ARTICLES_PER_CATEGORY:
                            all_articles_by_site_category[target_cat].append(processed_article)
                            print(f"      å·²æ·»åŠ  '{processed_article['title'][:30]}...' åˆ°åˆ†ç±» '{target_cat}' (æ¥è‡ª {feed_info['source_override']})")
                        else:
                            print(f"      åˆ†ç±» '{target_cat}' å·²æ»¡ (10æ¡)ï¼Œæ— æ³•æ·»åŠ æ¥è‡ª {feed_info['source_override']} çš„æ›´å¤šæ–‡ç« ã€‚")
                    else:
                        print(f"      è­¦å‘Š: æƒå¨æºæŒ‡å®šçš„ç›®æ ‡åˆ†ç±» '{target_cat}' ä¸åœ¨ç½‘ç«™åˆ†ç±»é…ç½®ä¸­ï¼Œè·³è¿‡ã€‚")
        time.sleep(1) # å‹å¥½è®¿é—®

    # --- æ­¥éª¤äºŒï¼šä» Google News RSS è·å–è¡¥å……æ–°é—» ---
    print("\n--- æ­£åœ¨ä» Google News RSS è·å–è¡¥å……æ–°é—» ---")
    for site_category_name, config in CATEGORIES_CONFIG.items():
        if len(all_articles_by_site_category[site_category_name]) < MAX_ARTICLES_PER_CATEGORY:
            print(f"  åˆ†ç±» '{site_category_name}' éœ€è¦è¡¥å……æ–°é—» (å½“å‰ {len(all_articles_by_site_category[site_category_name])} æ¡)ï¼Œå°†ä» Google News è·å–...")
            
            google_news_rss_url = f"https://news.google.com/rss/search?q={html.escape(config['keywords'])}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
            # æ³¨æ„ï¼šGoogle News çš„ source_override ç•™ç©ºï¼Œè®© fetch_articles_from_rss å°è¯•ä»æ ‡é¢˜æå–
            raw_articles_from_google = fetch_articles_from_rss(google_news_rss_url, source_name_override=None) 
            
            print(f"    å¤„ç†æ¥è‡ª Google News (åˆ†ç±»: {site_category_name}) çš„ {len(raw_articles_from_google)} æ¡åŸå§‹æ–°é—»...")
            for article_data in raw_articles_from_google:
                if len(all_articles_by_site_category[site_category_name]) >= MAX_ARTICLES_PER_CATEGORY:
                    break # è¿™ä¸ªåˆ†ç±»å·²ç»æ»¡äº†

                if article_data["url"] in seen_article_links:
                    print(f"      è·³è¿‡é‡å¤æ–‡ç«  (æ¥è‡ªGoogle News): {article_data['title'][:30]}...")
                    continue

                if is_within_last_month_rss(article_data["time_struct"], today):
                    time_display_str = "æœªçŸ¥æ—¶é—´"
                    if article_data["time_struct"]:
                        try:
                            time_display_str = time.strftime("%Y-%m-%d", article_data["time_struct"])
                        except: pass

                    processed_article = {
                        "title": article_data["title"],
                        "url": article_data["url"],
                        "snippet": article_data["snippet"],
                        "source": article_data["source"], # ä½¿ç”¨ fetch_articles_from_rss ä¸­å¤„ç†å¥½çš„ source
                        "time_display_str": time_display_str
                    }
                    all_articles_by_site_category[site_category_name].append(processed_article)
                    seen_article_links.add(article_data["url"])
                    print(f"        å·²æ·»åŠ  '{processed_article['title'][:30]}...' åˆ°åˆ†ç±» '{site_category_name}' (æ¥è‡ª Google News)")
            time.sleep(1) # å‹å¥½è®¿é—®
        else:
            print(f"  åˆ†ç±» '{site_category_name}' å·²æœ‰è¶³å¤Ÿæ–°é—» ({len(all_articles_by_site_category[site_category_name])}æ¡)ï¼Œè·³è¿‡ Google News è·å–ã€‚")

    # --- (5) ç”Ÿæˆæœ€ç»ˆçš„HTML ---
    final_html = generate_html_content(all_articles_by_site_category)
    
    output_filename = "index.html" 
    try:
        with open(output_filename, "w", encoding="utf-8") as f:
            f.write(final_html)
        print(f"\næˆåŠŸç”Ÿæˆç½‘é¡µï¼š{output_filename}")
    except IOError as e:
        print(f"\né”™è¯¯ï¼šæ— æ³•å†™å…¥æ–‡ä»¶ {output_filename}ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    except Exception as e:
        print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    print("èµ„è®¯ç½‘é¡µç”Ÿæˆå®Œæ¯•ã€‚")

