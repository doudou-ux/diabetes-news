# fetch_news.py
import datetime
import html
import os
import re # ç”¨äºè§„èŒƒåŒ–æ ‡é¢˜
import time
import requests
import feedparser
from bs4 import BeautifulSoup
from urllib.parse import urljoin # ç”¨äºçˆ¬è™«æ„å»ºå®Œæ•´URL
from deep_translator import GoogleTranslator # ç”¨äºç¿»è¯‘

# --- (1) é…ç½®æƒå¨ RSS æº ---
# æ·»åŠ  priority å­—æ®µï¼Œæ•°å€¼è¶Šå¤§è¶Šä¼˜å…ˆ
AUTHORITATIVE_RSS_FEEDS = [
    {
        "url": "https://www.medscape.com/rss/public/diabetes.xml",
        "source_override": "Medscape Diabetes",
        "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"],
        "priority": 10
    },
    {
        "url": "https://www.healio.com/news/endocrinology/rss",
        "source_override": "Healio Endocrinology",
        "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"],
        "priority": 9
    },
    {
        "url": "https://www.diabettech.com/feed/",
        "source_override": "Diabettech",
        "target_categories": ["æ²»ç–—è¿›å±•", "æœ€æ–°ç ”ç©¶"], # ç³–å°¿ç—…æŠ€æœ¯ç›¸å…³
        "priority": 8
    },
    {
        "url": "https://thesavvydiabetic.com/feed/",
        "source_override": "The Savvy Diabetic",
        "target_categories": ["æ‚£è€…æ•…äº‹ä¸å¿ƒç†æ”¯æŒ", "é¢„é˜²ä¸ç”Ÿæ´»æ–¹å¼"], # åšå®¢ï¼Œç»éªŒåˆ†äº«
        "priority": 7
    },
    {
        "url": "https://forum.diabetes.org.uk/boards/forums/-/index.rss",
        "source_override": "Diabetes UK è®ºå›",
        "target_categories": ["æ‚£è€…æ•…äº‹ä¸å¿ƒç†æ”¯æŒ"], # è®ºå›è®¨è®º
        "priority": 6
    },
    # --- è¯·æ‚¨æ›¿æ¢æˆ–æ·»åŠ ä»¥ä¸‹å ä½ç¬¦ï¼Œå¹¶æŒ‡å®š priority ---
    # {
    #     "url": "YOUR_PUBMED_DIABETES_RESEARCH_RSS_URL",
    #     "source_override": "PubMed",
    #     "target_categories": ["æœ€æ–°ç ”ç©¶"],
    #     "priority": 12
    # },
    # {
    #     "url": "YOUR_ADA_DIABETES_CARE_RSS_URL",
    #     "source_override": "Diabetes Care (ADA)",
    #     "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"],
    #     "priority": 11
    # },
]

# --- (1b) é…ç½®çˆ¬è™«æº ---
# æ³¨æ„ï¼šè¿™äº›çˆ¬è™«ç›®å‰ä¸æå–å‘å¸ƒæ—¥æœŸï¼Œå¯èƒ½æ— æ³•é€šè¿‡æ—¥æœŸè¿‡æ»¤
SCRAPED_SOURCES_CONFIG = [
    {
        "name": "Breakthrough T1D News",
        "fetch_function": "fetch_breakthrought1d_articles", # å¯¹åº”çš„çˆ¬è™«å‡½æ•°å
        "source_override": "Breakthrough T1D",
        "target_categories": ["æœ€æ–°ç ”ç©¶", "æ²»ç–—è¿›å±•"], # ä¸»è¦å…³äº1å‹ç³–å°¿ç—…ç ”ç©¶
        "priority": 8 # æƒå¨æœºæ„ï¼Œä½†çˆ¬è™«ç¨³å®šæ€§å¯èƒ½ç¨å·®
    },
    {
        "name": "MyGlu Articles",
        "fetch_function": "fetch_myglu_articles",
        "source_override": "MyGlu",
        "target_categories": ["æ‚£è€…æ•…äº‹ä¸å¿ƒç†æ”¯æŒ", "é¢„é˜²ä¸ç”Ÿæ´»æ–¹å¼"],
        "priority": 7
    },
    {
        "name": "DZD News (2025)", # å‡è®¾æ¯å¹´æ›´æ–°URLæˆ–çˆ¬è™«é€»è¾‘
        "fetch_function": "fetch_dzd_articles",
        "source_override": "DZD News",
        "target_categories": ["æœ€æ–°ç ”ç©¶"],
        "priority": 9 # å¾·å›½ç³–å°¿ç—…ç ”ç©¶ä¸­å¿ƒ
    },
]

GOOGLE_NEWS_PRIORITY = 1 # Google News ä½œä¸ºè¡¥å……æ¥æºï¼Œä¼˜å…ˆçº§è¾ƒä½

# --- (2) é…ç½®ç½‘ç«™å±•ç¤ºçš„åˆ†ç±»åŠå¯¹åº”çš„ Google News è¡¥å……å…³é”®è¯ ---
CATEGORIES_CONFIG = {
    "æœ€æ–°ç ”ç©¶": {
        "keywords": "ç³–å°¿ç—… æœ€æ–°è®ºæ–‡ OR ç³–å°¿ç—…æŠ€æœ¯çªç ´ OR ç³–å°¿ç—…æœºåˆ¶ç ”ç©¶ OR åŒ»å­¦ä¼šè®®ç³–å°¿ç—… OR GLP-1ç³–å°¿ç—… OR SGLT2ç³–å°¿ç—… OR èƒ°å²›Î²ç»†èƒ OR èƒ°å²›ç´ æ•æ„Ÿæ€§",
        "emoji": "ğŸ”¬"
    },
    "æ²»ç–—è¿›å±•": {
        "keywords": "ç³–å°¿ç—…æ–°è¯ OR ç³–å°¿ç—…é€‚åº”ç—‡æ‰©å±• OR ç³–å°¿ç—…è®¾å¤‡ç ”å‘ OR AIè¾…åŠ©è¯Šç–—ç³–å°¿ç—… OR è¾¾æ ¼åˆ—å‡€ OR å¸ç¾æ ¼é²è‚½ OR CGM OR è¿ç»­è¡€ç³–ç›‘æµ‹ OR èƒ°å²›ç´ æ³µ",
        "emoji": "ğŸ’Š"
    },
    "é¥®é£Ÿä¸è¥å…»": {
        "keywords": "ç³–å°¿ç—…é¥®é£ŸæŒ‡å— OR ç¢³æ°´äº¤æ¢è¡¨ OR ç³–å°¿ç—…é£Ÿè°± OR ä½GIé¥®é£Ÿç³–å°¿ç—… OR é«˜è›‹ç™½é¥®é£Ÿç³–å°¿ç—… OR é—´æ­‡æ€§æ–­é£Ÿç³–å°¿ç—… OR è†³é£Ÿçº¤ç»´ç³–å°¿ç—…",
        "emoji": "ğŸ¥—"
    },
    "é¢„é˜²ä¸ç”Ÿæ´»æ–¹å¼": {
        "keywords": "ç³–å°¿ç—…è¿åŠ¨å»ºè®® OR ç³–å°¿ç—…ç¡çœ  OR ç³–å°¿ç—…å‡é‡ OR æ§ç³–è®¡åˆ’ OR ç³–å°¿ç—…æ—©æœŸç­›æŸ¥ OR ç³–è€é‡å¼‚å¸¸ OR ä½“è„‚ç®¡ç†ç³–å°¿ç—… OR ç³–å°¿ç—…æ­¥æ•°ç›®æ ‡",
        "emoji": "ğŸƒâ€â™€ï¸"
    },
    "å¹¶å‘ç—‡ç®¡ç†": {
        "keywords": "ç³–å°¿ç—…è¶³ OR ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ OR ç³–å°¿ç—…è‚¾ç—… OR ç³–å°¿ç—…ç¥ç»ç—…å˜ OR ç³–ç½‘ç—… OR å¾®è¡€ç®¡ç—…å˜ç³–å°¿ç—… OR å°¿ç™½è›‹ç™½ç³–å°¿ç—…",
        "emoji": "ğŸ©º"
    },
    "æ‚£è€…æ•…äº‹ä¸å¿ƒç†æ”¯æŒ": {
        "keywords": "ç³–å°¿ç—…æ§ç³–ç»éªŒ OR ç³–å°¿ç—…å¿ƒç†æ”¯æŒ OR ç³–å°¿ç—…å®¶åº­æ”¯æŒ OR ç³–å°¿ç—…æ‚£è€…æ•…äº‹ OR ç³–å°¿ç—…åŒ»ç”Ÿé—®ç­”",
        "emoji": "ğŸ˜Š"
    },
    "æ”¿ç­–/åŒ»ä¿ä¿¡æ¯": {
        "keywords": "ç³–å°¿ç—…è¯å“çº³ä¿ OR ç³–å°¿ç—…åŒ»ä¿æŠ¥é”€ OR ç³–å°¿ç—…ç¤¾åŒºæ…¢ç—…éšè®¿ OR å›½å®¶è¯ç›‘å±€ç³–å°¿ç—…æ”¿ç­– OR åŒ»ä¿å±€ç³–å°¿ç—…æ”¿ç­–",
        "emoji": "ğŸ“„"
    }
}

# --- å¸®åŠ©å‡½æ•°ï¼šè§„èŒƒåŒ–æ ‡é¢˜ ---
def normalize_title(title):
    if not title: return ""
    title = title.lower()
    title = re.sub(r'[^\w\s-]', '', title)
    title = re.sub(r'\s+', ' ', title).strip()
    return title

# --- å¸®åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘ä¸€ä¸ªæœˆå†… ---
def is_within_last_month_rss(time_struct, today_date_obj):
    if not time_struct: return False # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆ™è®¤ä¸ºä¸åœ¨èŒƒå›´å†…
    try:
        article_date = datetime.date(time_struct.tm_year, time_struct.tm_mon, time_struct.tm_mday)
        thirty_days_ago = today_date_obj - datetime.timedelta(days=30)
        return thirty_days_ago <= article_date <= today_date_obj
    except Exception as e:
        print(f"      [is_within_last_month_rss] æ—¥æœŸè½¬æ¢é”™è¯¯: {e} - Time Struct: {time_struct}")
        return False

# --- å¸®åŠ©å‡½æ•°ï¼šæ¸…ç† HTML ---
def clean_html(raw_html):
    if not raw_html: return ""
    try: return BeautifulSoup(raw_html, "html.parser").get_text(separator=' ', strip=True)
    except Exception: return raw_html

# --- (A) RSS æºè·å–å‡½æ•° ---
def fetch_articles_from_rss(rss_url, source_name_override=None):
    # (æ­¤å‡½æ•°å†…å®¹ä¸ diabetes_news_fetch_dedup_priority ä¸­çš„ fetch_articles_from_rss åŸºæœ¬ç›¸åŒ)
    # ... (ä¸ºç®€æ´èµ·è§ï¼Œå‡è®¾æ­¤å‡½æ•°å·²æ­£ç¡®å®šä¹‰ï¼Œä¸ä¸Šä¸€ç‰ˆæœ¬ç›¸åŒ)
    print(f"    æ­£åœ¨ä» RSS æºè·å–: {rss_url} ({source_name_override or 'æœªçŸ¥æº'})")
    articles = []
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(rss_url, headers=headers, timeout=20)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        if not feed.entries:
            print(f"      æ­¤ RSS æºæœªè¿”å›ä»»ä½•æ¡ç›®: {rss_url}")
            return []
        print(f"      ä»æ­¤ RSS æºåŸå§‹è·å–åˆ° {len(feed.entries)} æ¡æ–°é—»ã€‚")
        for entry in feed.entries:
            title = entry.get("title", "æ— æ ‡é¢˜")
            link = entry.get("link", f"javascript:void(0);_{html.escape(title)}")
            published_time_struct = entry.get("published_parsed") or entry.get("updated_parsed")
            summary_html = entry.get("summary", entry.get("description", "æš‚æ— æ‘˜è¦"))
            snippet = clean_html(summary_html)
            actual_source_name = source_name_override
            if not actual_source_name:
                source_info = entry.get("source")
                actual_source_name = source_info.get("title") if source_info else "æœªçŸ¥æ¥æº"
                if "news.google.com" in rss_url and not source_name_override:
                    if ' - ' in title:
                        parts = title.rsplit(' - ', 1)
                        actual_source_name = parts[1]
            articles.append({
                "title": title, "url": link, "snippet": snippet,
                "source": actual_source_name, "time_struct": published_time_struct
            })
    except requests.exceptions.Timeout: print(f"      è·å– RSS æºæ—¶å‘ç”Ÿè¶…æ—¶é”™è¯¯: {rss_url}")
    except requests.exceptions.RequestException as e: print(f"      è·å– RSS æºæ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e} (URL: {rss_url})")
    except Exception as e: print(f"      å¤„ç† RSS æºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e} (URL: {rss_url})")
    return articles


# --- (B) çˆ¬è™«å‡½æ•°å®šä¹‰ ---
def translate_text(text, target_lang='zh-CN'): # zh-CN for Simplified Chinese
    if not text: return ""
    try:
        # GoogleTranslator(source='auto', target='zh') is deprecated, use 'zh-CN' or 'zh-TW'
        return GoogleTranslator(source='auto', target=target_lang).translate(text)
    except Exception as e:
        print(f"      ç¿»è¯‘é”™è¯¯: {e} - åŸæ–‡: {text[:50]}...")
        return text # ç¿»è¯‘å¤±è´¥åˆ™è¿”å›åŸæ–‡

def fetch_breakthrought1d_articles():
    BASE_URL = "https://www.breakthrought1d.org/news/"
    print(f"    æ­£åœ¨çˆ¬å–: {BASE_URL}")
    articles = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(BASE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        for article_el in soup.select("article"): # Element selector might need adjustment
            a_tag = article_el.select_one("h2 a")
            if not a_tag: continue

            title_en = a_tag.get_text(strip=True)
            link = urljoin(BASE_URL, a_tag.get("href"))
            summary_tag = article_el.select_one("p") # Selector for summary
            summary_en = summary_tag.get_text(strip=True) if summary_tag else ""
            
            # å°è¯•æå–æ—¥æœŸ - è¿™éƒ¨åˆ†éœ€è¦æ ¹æ®å®é™…ç½‘é¡µç»“æ„æ·»åŠ 
            date_str = None # Placeholder
            time_struct = None # Placeholder
            # Example: date_tag = article_el.select_one(".date-class")
            # if date_tag: date_str = date_tag.get_text(strip=True)
            # if date_str: try: time_struct = feedparser._parse_date(date_str) except: pass
            if not time_struct: print(f"      è­¦å‘Š: æœªèƒ½ä» {link} æå–å‘å¸ƒæ—¥æœŸã€‚")


            title_cn = translate_text(title_en)
            summary_cn = translate_text(summary_en)

            articles.append({
                "title": title_cn, "url": link, "snippet": summary_cn,
                "source": "Breakthrough T1D", # åœ¨ SCRAPED_SOURCES_CONFIG ä¸­å·²å®šä¹‰
                "time_struct": time_struct # ç›®å‰ä¸º None
            })
    except Exception as e:
        print(f"      çˆ¬å– Breakthrough T1D æ—¶å‡ºé”™: {e}")
    return articles

def fetch_myglu_articles():
    BASE_URL = "https://myglu.org/articles"
    print(f"    æ­£åœ¨çˆ¬å–: {BASE_URL}")
    articles = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(BASE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        for item in soup.select("div.node-article"): # Element selector
            a_tag = item.select_one("h2 a")
            if not a_tag: continue

            title_en = a_tag.get_text(strip=True)
            link = urljoin(BASE_URL, a_tag.get("href"))
            summary_tag = item.select_one("div.field-name-body div.field-item") # More specific selector
            summary_en = summary_tag.get_text(strip=True) if summary_tag else ""

            date_str = None
            time_struct = None
            # Example: date_tag = item.select_one("span.date-display-single")
            # if date_tag: date_str = date_tag.get_text(strip=True)
            # if date_str: try: time_struct = feedparser._parse_date(date_str) except: pass
            if not time_struct: print(f"      è­¦å‘Š: æœªèƒ½ä» {link} æå–å‘å¸ƒæ—¥æœŸã€‚")


            title_cn = translate_text(title_en)
            summary_cn = translate_text(summary_en)
            
            articles.append({
                "title": title_cn, "url": link, "snippet": summary_cn,
                "source": "MyGlu", # åœ¨ SCRAPED_SOURCES_CONFIG ä¸­å·²å®šä¹‰
                "time_struct": time_struct # ç›®å‰ä¸º None
            })
    except Exception as e:
        print(f"      çˆ¬å– MyGlu æ—¶å‡ºé”™: {e}")
    return articles

def fetch_dzd_articles():
    BASE_URL = "https://www.dzd-ev.de/en/press/press-releases/press-releases-2025/index.html" # URL may need annual update
    print(f"    æ­£åœ¨çˆ¬å–: {BASE_URL}")
    articles = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(BASE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        for item in soup.select("div.teaser-text"): # Element selector
            title_tag = item.select_one("a")
            if not title_tag: continue

            title_en = title_tag.get_text(strip=True)
            link = urljoin(BASE_URL, title_tag.get("href"))
            summary_tag = item.select_one("p")
            summary_en = summary_tag.get_text(strip=True) if summary_tag else ""

            date_str = None
            time_struct = None
            # Example: date_container = item.find_previous_sibling("div", class_="news-list-date")
            # if date_container: date_str = date_container.get_text(strip=True)
            # if date_str: try: time_struct = feedparser._parse_date(date_str) except: pass
            if not time_struct: print(f"      è­¦å‘Š: æœªèƒ½ä» {link} æå–å‘å¸ƒæ—¥æœŸã€‚")

            title_cn = translate_text(title_en)
            summary_cn = translate_text(summary_en)

            articles.append({
                "title": title_cn, "url": link, "snippet": summary_cn,
                "source": "DZD News", # åœ¨ SCRAPED_SOURCES_CONFIG ä¸­å·²å®šä¹‰
                "time_struct": time_struct # ç›®å‰ä¸º None
            })
    except Exception as e:
        print(f"      çˆ¬å– DZD News æ—¶å‡ºé”™: {e}")
    return articles

# å°†å‡½æ•°åå­—ç¬¦ä¸²æ˜ å°„åˆ°å®é™…å‡½æ•°å¯¹è±¡
SCRAPER_FUNCTIONS_MAP = {
    "fetch_breakthrought1d_articles": fetch_breakthrought1d_articles,
    "fetch_myglu_articles": fetch_myglu_articles,
    "fetch_dzd_articles": fetch_dzd_articles,
}


# --- HTML ç”Ÿæˆé€»è¾‘ (ä¸ä¹‹å‰ç‰ˆæœ¬ diabetes_news_fetch_tabs_v1 ä¸€è‡´) ---
def generate_html_content(all_news_data_sorted):
    # (æ­¤å‡½æ•°å†…å®¹ä¸ diabetes_news_fetch_tabs_v1 ä¸­çš„ generate_html_content å®Œå…¨ç›¸åŒï¼Œæ­¤å¤„çœç•¥ä»¥å‡å°‘é‡å¤)
    # è¯·ç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯ diabetes_news_fetch_tabs_v1 ä¸­çš„å®Œæ•´ generate_html_content å‡½æ•°
    current_time_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
    app_timezone = os.getenv('APP_TIMEZONE', 'UTC')
    if app_timezone != 'UTC':
        try:
             current_time_str = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8))).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S %Z')
        except Exception as e:
            print(f"åº”ç”¨æ—¶åŒº ({app_timezone}) æ—¶å‡ºé”™: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤æœåŠ¡å™¨æ—¶é—´ã€‚")
            current_time_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (æœåŠ¡å™¨æ—¶é—´)')

    current_year = datetime.datetime.now().year
    github_repo_url = f"https://github.com/{os.getenv('GITHUB_REPOSITORY', 'doudou-ux/diabetes-news')}"
    html_output = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç³–å°¿ç—…å‰æ²¿èµ„è®¯</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {{ font-family: 'Inter', sans-serif; background-color: #f0f4f8; }}
        .news-card {{ transition: transform 0.3s ease, box-shadow 0.3s ease; display: flex; flex-direction: column; justify-content: space-between; height: 100%; background-color: #ffffff; border-radius: 0.5rem; }}
        .news-card:hover {{ transform: translateY(-5px); box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); }}
        .tab-content {{ background-color: #ffffff; border-radius: 0.75rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); padding: 1.25rem; display: none; }}
        .tab-content.active {{ display: block; }}
        @media (min-width: 768px) {{ .tab-content {{ padding: 2rem; }} }}
        .category-title-text {{ border-bottom: 3px solid #3b82f6; padding-bottom: 0.75rem; margin-bottom: 1.75rem; color: #1e40af; font-size: 1.5rem; }}
        @media (min-width: 768px) {{ .category-title-text {{ font-size: 1.875rem; }} }}
        .news-item-link {{ color: #2563eb; text-decoration: none; font-weight: 600; }}
        .news-item-link:hover {{ text-decoration: underline; color: #1d4ed8; }}
        .source-tag, .time-tag {{ padding: 0.25rem 0.75rem; border-radius: 0.375rem; font-size: 0.75rem; font-weight: 500; line-height: 1.25; }}
        .source-tag {{ background-color: #e0e7ff; color: #3730a3; }}
        .time-tag {{ background-color: #d1fae5; color: #065f46; }}
        .card-content {{ flex-grow: 1; }}
        .card-footer {{ margin-top: auto; }}
        .header-main-title {{ font-size: 1.875rem; }}
        @media (min-width: 640px) {{ .header-main-title {{ font-size: 2.25rem; }} }}
        @media (min-width: 768px) {{ .header-main-title {{ font-size: 3rem; }} }}
        .main-container {{ padding: 1rem; }}
        @media (min-width: 768px) {{ .main-container {{ padding: 2rem; }} }}
        .loader {{ border: 5px solid #f3f3f3; border-radius: 50%; border-top: 5px solid #3498db; width: 50px; height: 50px; -webkit-animation: spin 1s linear infinite; animation: spin 1s linear infinite; margin: 20px auto; }}
        @-webkit-keyframes spin {{ 0% {{ -webkit-transform: rotate(0deg); }} 100% {{ -webkit-transform: rotate(360deg); }} }}
        @keyframes spin {{ 0% {{ transform: rotate(0deg); }} 100% {{ transform: rotate(360deg); }} }}
        .tab-buttons-container {{ display: flex; flex-wrap: wrap; margin-bottom: 1.5rem; border-bottom: 2px solid #d1d5db; }}
        .tab-button {{ padding: 0.75rem 1.25rem; margin-right: 0.5rem; margin-bottom: -2px; border: 2px solid transparent; border-bottom: none; border-top-left-radius: 0.375rem; border-top-right-radius: 0.375rem; cursor: pointer; font-weight: 500; color: #4b5563; background-color: #f9fafb; transition: all 0.3s ease; }}
        .tab-button:hover {{ color: #1f2937; background-color: #f3f4f6; }}
        .tab-button.active {{ color: #1d4ed8; background-color: #ffffff; border-color: #d1d5db; border-bottom-color: #ffffff; }}
    </style>
</head>
<body class="bg-gray-100 text-gray-800">
    <div class="container mx-auto main-container">
        <header class="text-center mb-10 md:mb-16">
            <h1 class="font-bold text-blue-700 header-main-title">ç³–å°¿ç—…å‰æ²¿èµ„è®¯</h1>
            <p class="text-gray-600 mt-3 text-base md:text-lg">æœ€è¿‘ä¸€ä¸ªæœˆåŠ¨æ€ï¼ˆè‡ªåŠ¨æ›´æ–°äºï¼š<span id="updateTime">{current_time_str}</span>ï¼‰</p>
            <p class="text-sm text-gray-500 mt-2">èµ„è®¯ç»¼åˆæ¥æº</p>
        </header>
        <div class="tab-buttons-container" id="tabButtons">"""
    first_category = True
    for category_name_key in all_news_data_sorted.keys():
        category_config = CATEGORIES_CONFIG.get(category_name_key, {})
        emoji = category_config.get("emoji", "")
        tab_id = "tab-" + html.escape(category_name_key.replace(" ", "-").replace("/", "-").lower())
        active_class = "active" if first_category else ""
        html_output += f"""<button class="tab-button {active_class}" data-tab-target="#{tab_id}">{emoji} {html.escape(category_name_key)}</button>"""
        first_category = False
    html_output += """</div><div id="news-content">"""
    first_category = True
    if not any(all_news_data_sorted.values()):
        html_output += '<p class="text-center text-gray-500 text-xl py-10">æŠ±æ­‰ï¼Œç›®å‰æœªèƒ½åŠ è½½åˆ°æœ€è¿‘ä¸€ä¸ªæœˆç›¸å…³çš„ç³–å°¿ç—…èµ„è®¯ã€‚</p>'
    else:
        for category_name_key, articles in all_news_data_sorted.items():
            category_config = CATEGORIES_CONFIG.get(category_name_key, {})
            emoji = category_config.get("emoji", "")
            tab_id = "tab-" + html.escape(category_name_key.replace(" ", "-").replace("/", "-").lower())
            active_class = "active" if first_category else ""
            category_html_content = f"""<div id="{tab_id}" class="tab-content {active_class}"><h2 class="font-semibold category-title-text">{emoji} {html.escape(category_name_key)}</h2>"""
            if not articles:
                category_html_content += '<p class="text-gray-500">æœ€è¿‘ä¸€ä¸ªæœˆæš‚æ— è¯¥åˆ†ç±»ä¸‹çš„èµ„è®¯ã€‚</p>'
            else:
                category_html_content += '<div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6 md:gap-8 news-grid">'
                for article in articles: 
                    title = html.escape(article.get('title', 'æ— æ ‡é¢˜'))
                    url = html.escape(article.get('url', 'javascript:void(0);'))
                    snippet_raw = article.get('snippet', 'æš‚æ— æ‘˜è¦')
                    snippet = html.escape(snippet_raw[:150] + ('...' if len(snippet_raw) > 150 else ''))
                    source_display = html.escape(article.get('source', 'æœªçŸ¥æ¥æº'))
                    time_display = html.escape(article.get('time_display_str', 'æœªçŸ¥æ—¶é—´'))
                    category_html_content += f"""<div class="news-card shadow-md hover:shadow-lg p-5"><div class="card-content"><h3 class="text-lg mb-2"><a href="{url}" {'target="_blank" rel="noopener noreferrer"' if url != 'javascript:void(0);' else ''} class="news-item-link">{title}</a></h3><p class="text-gray-600 text-sm mb-4 leading-relaxed">{snippet}</p></div><div class="card-footer text-xs text-gray-500 flex flex-wrap gap-2 items-center pt-3 border-t border-gray-200"><span class="source-tag">{source_display}</span><span class="time-tag">{time_display}</span></div></div>"""
                category_html_content += "</div>" 
            category_html_content += "</div>"
            html_output += category_html_content
            first_category = False
    html_output += f"""</div> </div> <footer class="text-center p-6 mt-12 text-gray-600 text-sm border-t border-gray-300"><p>&copy; {current_year} ç³–å°¿ç—…èµ„è®¯èšåˆ. <a href="{github_repo_url}" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">é¡¹ç›®æºç </a></p><p class="mt-1">æœ¬ç«™å†…å®¹ä»…ä¾›å‚è€ƒ, ä¸æ„æˆåŒ»ç–—å»ºè®®ã€‚</p></footer>
    <script>
        document.addEventListener('DOMContentLoaded', function () {{
            const tabButtons = document.querySelectorAll('.tab-button');
            const tabContents = document.querySelectorAll('.tab-content');
            if (tabButtons.length > 0 && tabContents.length > 0) {{
                tabButtons.forEach(button => {{
                    button.addEventListener('click', () => {{
                        tabButtons.forEach(btn => btn.classList.remove('active'));
                        tabContents.forEach(content => content.classList.remove('active'));
                        button.classList.add('active');
                        const targetContentId = button.dataset.tabTarget;
                        const targetContent = document.querySelector(targetContentId);
                        if (targetContent) {{ targetContent.classList.add('active'); }}
                    }});
                }});
            }}
        }});
    </script>
</body></html>"""
    return html_output

# --- (5) ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == "__main__":
    print("å¼€å§‹ä»å¤šä¸ª RSS æºå’Œçˆ¬è™«ç”Ÿæˆç³–å°¿ç—…èµ„è®¯ç½‘é¡µ...")
    
    unique_articles_candidates = {}
    globally_seen_urls = set()
    today = datetime.date.today()
    MAX_ARTICLES_PER_CATEGORY = 10

    # --- æ­¥éª¤ä¸€ï¼šä»æƒå¨ RSS æºè·å–æ–°é—» ---
    print("\n--- æ­£åœ¨ä»æƒå¨ RSS æºè·å–æ–°é—» ---")
    for feed_info in AUTHORITATIVE_RSS_FEEDS:
        current_priority = feed_info.get("priority", 5) 
        raw_articles_from_feed = fetch_articles_from_rss(feed_info["url"], feed_info["source_override"])
        for article_data in raw_articles_from_feed:
            if article_data["url"] in globally_seen_urls: continue
            if is_within_last_month_rss(article_data["time_struct"], today):
                normalized_title = normalize_title(article_data["title"])
                time_display_str = "æœªçŸ¥æ—¶é—´"
                if article_data["time_struct"]:
                    try: time_display_str = time.strftime("%Y-%m-%d", article_data["time_struct"])
                    except: pass
                article_obj_for_storage = {
                    "title": article_data["title"], "url": article_data["url"],
                    "snippet": article_data["snippet"], "source": article_data["source"],
                    "time_display_str": time_display_str, "time_struct": article_data["time_struct"],
                    "source_priority": current_priority
                }
                if normalized_title not in unique_articles_candidates or \
                   current_priority > unique_articles_candidates[normalized_title]["priority"]:
                    unique_articles_candidates[normalized_title] = {
                        "article_obj": article_obj_for_storage, "priority": current_priority,
                        "target_categories": feed_info["target_categories"], "url": article_data["url"]
                    }
                    print(f"      å€™é€‰(æƒå¨RSS): '{article_obj_for_storage['title'][:30]}...' (Prio: {current_priority})")
        time.sleep(1)

    # --- æ­¥éª¤äºŒï¼šä»çˆ¬è™«æºè·å–æ–°é—» ---
    print("\n--- æ­£åœ¨ä»çˆ¬è™«æºè·å–æ–°é—» ---")
    for scraper_info in SCRAPED_SOURCES_CONFIG:
        fetch_function = SCRAPER_FUNCTIONS_MAP.get(scraper_info["fetch_function"])
        if not fetch_function:
            print(f"    é”™è¯¯: æœªæ‰¾åˆ°çˆ¬è™«å‡½æ•° {scraper_info['fetch_function']}")
            continue
        
        current_priority = scraper_info.get("priority", 3) # çˆ¬è™«æºé»˜è®¤ä¼˜å…ˆçº§ä¸º3
        raw_articles_from_scraper = fetch_function() # è°ƒç”¨å¯¹åº”çš„çˆ¬è™«å‡½æ•°
        
        for article_data in raw_articles_from_scraper: # article_data ç»“æ„åº”ä¸RSSè·å–çš„ç±»ä¼¼
            if article_data["url"] in globally_seen_urls: continue
            # æ³¨æ„ï¼šçˆ¬è™«è·å–çš„æ–‡ç« ç›®å‰ time_struct ä¸º Noneï¼Œæ‰€ä»¥ is_within_last_month_rss ä¼šè¿”å› False
            # å¦‚æœå¸Œæœ›å®ƒä»¬é€šè¿‡ï¼Œéœ€è¦ä¿®æ”¹ is_within_last_month_rss æˆ–ç¡®ä¿çˆ¬è™«èƒ½æå–æ—¥æœŸ
            if is_within_last_month_rss(article_data["time_struct"], today): # è¿™è¡Œå¯èƒ½å¯¼è‡´çˆ¬è™«å†…å®¹è¢«è¿‡æ»¤
                normalized_title = normalize_title(article_data["title"])
                # time_display_str å¯¹äºçˆ¬è™«å†…å®¹ï¼Œå¦‚æœtime_structä¸ºNoneï¼Œåˆ™ä¼šæ˜¯"æœªçŸ¥æ—¶é—´"
                time_display_str = "æœªçŸ¥æ—¶é—´"
                if article_data.get("time_struct"): # ç¡®ä¿æœ‰ time_struct
                    try: time_display_str = time.strftime("%Y-%m-%d", article_data["time_struct"])
                    except: pass
                else: # å¦‚æœçˆ¬è™«æ²¡æœ‰æä¾›æ—¥æœŸï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ä¸€ä¸ªå›ºå®šæ–‡æœ¬æˆ–å½“å¤©æ—¥æœŸ
                    # time_display_str = f"æŠ“å–äº {today.strftime('%Y-%m-%d')}" # ç¤ºä¾‹
                    print(f"      æ³¨æ„: æ–‡ç«  '{article_data['title'][:30]}...' æ— æœ‰æ•ˆå‘å¸ƒæ—¥æœŸï¼Œå°†æ˜¾ç¤º'æœªçŸ¥æ—¶é—´'")


                article_obj_for_storage = {
                    "title": article_data["title"], "url": article_data["url"],
                    "snippet": article_data["snippet"], "source": scraper_info["source_override"],
                    "time_display_str": time_display_str, "time_struct": article_data["time_struct"],
                    "source_priority": current_priority
                }
                if normalized_title not in unique_articles_candidates or \
                   current_priority > unique_articles_candidates[normalized_title]["priority"]:
                    unique_articles_candidates[normalized_title] = {
                        "article_obj": article_obj_for_storage, "priority": current_priority,
                        "target_categories": scraper_info["target_categories"], "url": article_data["url"]
                    }
                    print(f"      å€™é€‰(çˆ¬è™«): '{article_obj_for_storage['title'][:30]}...' (Prio: {current_priority})")
            else:
                 print(f"      è·³è¿‡(çˆ¬è™«ï¼Œæ—¥æœŸè¿‡æ»¤): '{article_data['title'][:30]}...' (Time struct: {article_data.get('time_struct')})")

        time.sleep(1)


    # --- æ­¥éª¤ä¸‰ï¼šä» Google News RSS è·å–è¡¥å……æ–°é—» ---
    print("\n--- æ­£åœ¨ä» Google News RSS è·å–è¡¥å……æ–°é—» ---")
    for site_category_name, config in CATEGORIES_CONFIG.items():
        print(f"  ä¸ºåˆ†ç±» '{site_category_name}' ä» Google News è·å–è¡¥å……...")
        google_news_rss_url = f"https://news.google.com/rss/search?q={html.escape(config['keywords'])}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        raw_articles_from_google = fetch_articles_from_rss(google_news_rss_url, source_name_override=None)
        
        for article_data in raw_articles_from_google:
            if article_data["url"] in globally_seen_urls and \
               any(article_data["url"] == cand["url"] for cand in unique_articles_candidates.values()):
                continue
            if is_within_last_month_rss(article_data["time_struct"], today):
                normalized_title = normalize_title(article_data["title"])
                time_display_str = "æœªçŸ¥æ—¶é—´"
                if article_data["time_struct"]:
                    try: time_display_str = time.strftime("%Y-%m-%d", article_data["time_struct"])
                    except: pass
                article_obj_for_storage = {
                    "title": article_data["title"], "url": article_data["url"],
                    "snippet": article_data["snippet"], "source": article_data["source"],
                    "time_display_str": time_display_str, "time_struct": article_data["time_struct"],
                    "source_priority": GOOGLE_NEWS_PRIORITY
                }
                if normalized_title not in unique_articles_candidates or \
                   GOOGLE_NEWS_PRIORITY > unique_articles_candidates[normalized_title]["priority"]:
                    unique_articles_candidates[normalized_title] = {
                        "article_obj": article_obj_for_storage, "priority": GOOGLE_NEWS_PRIORITY,
                        "target_categories": [site_category_name], "url": article_data["url"]
                    }
                    print(f"      å€™é€‰(Google): '{article_obj_for_storage['title'][:30]}...' (Prio: {GOOGLE_NEWS_PRIORITY}) for category {site_category_name}")
        time.sleep(1)

    # --- æ­¥éª¤å››ï¼šå°† unique_articles_candidates åˆ†é…åˆ°æœ€ç»ˆçš„åˆ†ç±»å­—å…¸ä¸­ ---
    print("\n--- æ­£åœ¨å°†å»é‡å’Œä¼˜å…ˆé€‰æ‹©åçš„æ–°é—»åˆ†é…åˆ°å„åˆ†ç±» ---")
    all_articles_by_site_category_temp = {category_name: [] for category_name in CATEGORIES_CONFIG.keys()}
    for candidate_info in unique_articles_candidates.values():
        article_to_add = candidate_info["article_obj"]
        if article_to_add["url"] not in globally_seen_urls:
            for target_cat in candidate_info["target_categories"]:
                if target_cat in all_articles_by_site_category_temp:
                    all_articles_by_site_category_temp[target_cat].append(article_to_add)
            globally_seen_urls.add(article_to_add["url"])

    # --- æ­¥éª¤äº”ï¼šå¯¹æ¯ä¸ªåˆ†ç±»çš„æ–‡ç« æŒ‰æ—¥æœŸæ’åºå¹¶æˆªå– ---
    print("\n--- æ­£åœ¨å¯¹å„åˆ†ç±»æ–°é—»è¿›è¡Œæ’åºå’Œæˆªå– ---")
    all_articles_by_site_category_final_sorted = {}
    for category_name, articles_list in all_articles_by_site_category_temp.items():
        articles_list.sort(key=lambda x: time.mktime(x["time_struct"]) if x["time_struct"] else -float('inf'), reverse=True)
        all_articles_by_site_category_final_sorted[category_name] = articles_list[:MAX_ARTICLES_PER_CATEGORY]
        print(f"  åˆ†ç±» '{category_name}' æ’åºå¹¶æˆªå–åæœ‰ {len(all_articles_by_site_category_final_sorted[category_name])} æ¡æ–°é—»ã€‚")

    # --- (6) ç”Ÿæˆæœ€ç»ˆçš„HTML ---
    final_html = generate_html_content(all_articles_by_site_category_final_sorted)
    
    output_filename = "index.html" 
    try:
        with open(output_filename, "w", encoding="utf-8") as f:
            f.write(final_html)
        print(f"\næˆåŠŸç”Ÿæˆç½‘é¡µï¼š{output_filename}")
    except IOError as e:
        print(f"\né”™è¯¯ï¼šæ— æ³•å†™å…¥æ–‡ä»¶ {output_filename}ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    except Exception as e:
        print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    print("èµ„è®¯ç½‘é¡µç”Ÿæˆå®Œæ¯•ã€‚")
