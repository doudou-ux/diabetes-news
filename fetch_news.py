import datetime
import html
import os
import re
import time
import requests # ä½¿ç”¨ requests è¿›è¡Œ HTTP è°ƒç”¨
import json
import feedparser
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote_plus
from dateutil.parser import parse as dateutil_parse # ä½¿ç”¨ dateutil è§£ææ—¥æœŸæ›´çµæ´»

try:
    from Bio import Entrez # For PubMed API
except ImportError:
    print("é”™è¯¯ï¼šæœªæ‰¾åˆ° Biopython åº“ã€‚è¯·é€šè¿‡ 'pip install biopython' å®‰è£…ã€‚")
    Entrez = None # Set Entrez to None if import fails

# --- (0) ä»ç¯å¢ƒå˜é‡è¯»å–è®¯é£æ˜Ÿç« API Keys ---
SPARK_API_PASSWORD = os.getenv("SPARK_API_PASSWORD")
SPARK_LITE_HTTP_URL = "https://spark-api-open.xf-yun.com/v1/chat/completions"
SPARK_APPID = os.getenv("SPARK_APPID")
SPARK_API_KEY = os.getenv("SPARK_API_KEY")
SPARK_API_SECRET = os.getenv("SPARK_API_SECRET")

# --- (1) é…ç½®æƒå¨ RSS æº ---
AUTHORITATIVE_RSS_FEEDS = [
    # {"url": "https://www.medscape.com/cx/rss/professional.xml", "source_override": "Medscape Professional", "priority": 10, "needs_translation": True}, # Original - 404
    {"url": "http://rss.medscape.com/medscapetoday.rss", "source_override": "Medscape Today", "priority": 10, "needs_translation": True}, # å°è¯•æ–°çš„ Medscape é“¾æ¥
    {"url": "https://www.healio.com/sws/feed/news/endocrinology", "source_override": "Healio Endocrinology", "priority": 9, "needs_translation": True},
    {"url": "https://www.diabettech.com/feed/", "source_override": "Diabettech", "priority": 8, "needs_translation": True},
    {"url": "https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/press-releases/rss.xml", "source_override": "FDA (US) Press", "priority": 10, "needs_translation": True}, # FDA é“¾æ¥å¯èƒ½ä»ä¼šå¤±è´¥
]

# --- (1b) é…ç½®çˆ¬è™«æº ---
SCRAPED_SOURCES_CONFIG = [
    {"name": "Breakthrough T1D News", "fetch_function": "fetch_breakthrought1d_articles", "source_override": "Breakthrough T1D", "priority": 8},
    {"name": "DZD News (2025)", "fetch_function": "fetch_dzd_articles", "source_override": "DZD News", "priority": 9},
    {"name": "PubMed API Search", "fetch_function": "fetch_pubmed_articles", "source_override": "PubMed", "priority": 12},
    {"name": "IDF News", "fetch_function": "fetch_idf_articles", "source_override": "IDF News", "priority": 9},
]

GOOGLE_NEWS_PRIORITY = 1
SOURCE_TYPE_ORDER = {'authoritative_rss': 0, 'scraper': 1, 'google_news': 2, 'unknown': 99}

# --- (2) é…ç½®ç½‘ç«™å±•ç¤ºçš„åˆ†ç±» ---
CATEGORIES_CONFIG = {
    "æœ€æ–°ç ”ç©¶": {"emoji": "ğŸ”¬"}, "æ²»ç–—è¿›å±•": {"emoji": "ğŸ’Š"}, "é¥®é£Ÿä¸è¥å…»": {"emoji": "ğŸ¥—"},
    "é¢„é˜²ä¸ç”Ÿæ´»æ–¹å¼": {"emoji": "ğŸƒâ€â™€ï¸"}, "å¹¶å‘ç—‡ç®¡ç†": {"emoji": "ğŸ©º"}, "æ‚£è€…æ•…äº‹ä¸å¿ƒç†æ”¯æŒ": {"emoji": "ğŸ˜Š"},
    "æ”¿ç­–/åŒ»ä¿ä¿¡æ¯": {"emoji": "ğŸ“„"}, "ç»¼åˆèµ„è®¯": {"emoji": "ğŸ“°"}
}
VALID_CATEGORY_NAMES = list(CATEGORIES_CONFIG.keys())
if "ç»¼åˆèµ„è®¯" in VALID_CATEGORY_NAMES: VALID_CATEGORY_NAMES.remove("ç»¼åˆèµ„è®¯")

# --- å¸®åŠ©å‡½æ•°ï¼šè§„èŒƒåŒ–æ ‡é¢˜ ---
def normalize_title(title):
    if not title: return ""
    title = title.lower(); title = re.sub(r'[^\w\s-]', '', title); title = re.sub(r'\s+', ' ', title).strip()
    return title

# --- å¸®åŠ©å‡½æ•°ï¼šå°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸² ---
def parse_date_flexible(date_str):
    """å°è¯•ç”¨å¤šç§æ–¹å¼è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å› time.struct_time æˆ– None"""
    if not date_str:
        return None
    try:
        # dateutil.parser å¯ä»¥å¤„ç†å¤šç§æ ¼å¼
        dt_obj = dateutil_parse(date_str)
        return dt_obj.timetuple()
    except Exception as e:
        # print(f"      ä½¿ç”¨ dateutil è§£ææ—¥æœŸå¤±è´¥: {date_str} - {e}") # å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥è°ƒè¯•æ—¥æœŸè§£æ
        # å¯ä»¥æ·»åŠ æ›´å¤šç‰¹å®šæ ¼å¼çš„å°è¯•ï¼Œå¦‚æœ dateutil å¤±è´¥çš„è¯
        # try:
        #     dt_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
        #     return dt_obj.timetuple()
        # except ValueError:
        #     pass
        return None # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥

# --- å¸®åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘ä¸€ä¸ªæœˆå†… (ä½¿ç”¨ time.struct_time) ---
def is_within_last_month(time_struct, today_date_obj):
    """æ£€æŸ¥ time.struct_time å¯¹è±¡æ˜¯å¦åœ¨æœ€è¿‘30å¤©å†…"""
    if not time_struct:
        # print("      æ—¥æœŸç»“æ„ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ¤æ–­æ˜¯å¦åœ¨æœ€è¿‘ä¸€æœˆå†…ã€‚") # å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥è°ƒè¯•
        return False
    try:
        # ç¡®ä¿ time_struct è‡³å°‘åŒ…å«å¹´ã€æœˆã€æ—¥ä¿¡æ¯
        if not all(hasattr(time_struct, attr) for attr in ['tm_year', 'tm_mon', 'tm_mday']):
             print(f"      æ—¥æœŸç»“æ„ä¸å®Œæ•´: {time_struct}ï¼Œæ— æ³•åˆ¤æ–­æ˜¯å¦åœ¨æœ€è¿‘ä¸€æœˆå†…ã€‚")
             return False # å¦‚æœç¼ºå°‘å¿…è¦å±æ€§

        article_date = datetime.date(time_struct.tm_year, time_struct.tm_mon, time_struct.tm_mday)
        thirty_days_ago = today_date_obj - datetime.timedelta(days=30)
        is_recent = thirty_days_ago <= article_date <= today_date_obj
        # if not is_recent:
        #      print(f"      æ–‡ç« æ—¥æœŸ {article_date} ä¸åœ¨æœ€è¿‘30å¤©å†…ã€‚") # å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥è°ƒè¯•
        return is_recent
    except ValueError as ve:
        # å¤„ç†æ— æ•ˆæ—¥æœŸï¼Œä¾‹å¦‚ 2 æœˆ 30 æ—¥
        print(f"      è§£ææ—¥æœŸæ—¶å‘ç”Ÿå€¼é”™è¯¯: {time_struct} - {ve}")
        return False
    except Exception as e:
        print(f"      åˆ¤æ–­æ—¥æœŸæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {time_struct} - {e}")
        return False

# --- å¸®åŠ©å‡½æ•°ï¼šæ¸…ç† HTML ---
def clean_html(raw_html):
    if not raw_html: return ""
    try: return BeautifulSoup(raw_html, "html.parser").get_text(separator=' ', strip=True)
    except Exception: return raw_html

# --- ç¿»è¯‘å‡½æ•° (ä½¿ç”¨è®¯é£æ˜Ÿç« HTTP API - Bearer Token Auth) ---
def translate_text_with_llm(text, target_lang='Chinese'):
    global llm_call_count, MAX_LLM_CALLS # å¼•ç”¨å…¨å±€å˜é‡
    if not text or not isinstance(text, str) or not text.strip(): return text
    if not SPARK_API_PASSWORD:
        print("      é”™è¯¯: è®¯é£æ˜Ÿç« APIPassword æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œ LLM ç¿»è¯‘ã€‚")
        return text
    if llm_call_count >= MAX_LLM_CALLS:
        # print(f"      è­¦å‘Š: å·²è¾¾åˆ° LLM è°ƒç”¨æ¬¡æ•°ä¸Šé™ ({MAX_LLM_CALLS})ï¼Œè·³è¿‡ç¿»è¯‘ã€‚") # å‡å°‘å†—ä½™æ‰“å°
        return text # è¿”å›åŸæ–‡

    max_input_length = 500
    text_to_translate = text[:max_input_length]
    prompt = f"Please translate the following text to {target_lang}. Only return the translated text, without any introduction or explanation.\n\nText to translate:\n{text_to_translate}"
    # print(f"      æ­£åœ¨è°ƒç”¨è®¯é£æ˜Ÿç« HTTP API ç¿»è¯‘: {text_to_translate[:30]}...") # å‡å°‘å†—ä½™æ‰“å°
    llm_call_count += 1

    try:
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {SPARK_API_PASSWORD}"}
        payload = {"model": "lite", "messages": [{"role": "user", "content": prompt}], "temperature": 0.3, "max_tokens": int(len(text_to_translate) * 1.5) + 50}
        response = requests.post(SPARK_LITE_HTTP_URL, headers=headers, json=payload, timeout=20)
        response.raise_for_status()
        response_data = response.json()
        llm_output = ""
        if 'choices' in response_data and response_data['choices']:
            message = response_data['choices'][0].get('message', {})
            llm_output = message.get('content', '').strip()
        elif 'payload' in response_data and 'choices' in response_data['payload'] and \
             'text' in response_data['payload']['choices'] and response_data['payload']['choices']['text']:
            llm_output = response_data['payload']['choices']['text'][0].get('content', '').strip()
        else:
            print(f"      è­¦å‘Š: æœªçŸ¥çš„è®¯é£æ˜Ÿç« API ç¿»è¯‘å“åº”ç»“æ„: {response_data}")
            return text
        
        cleaned_output = llm_output.strip().strip('"').strip("'")
        if cleaned_output and cleaned_output.lower() != text_to_translate.lower(): # ç¡®ä¿ç¿»è¯‘ç»“æœéç©ºä¸”ä¸åŸæ–‡ä¸åŒ
            # print(f"      ç¿»è¯‘æˆåŠŸ: {text_to_translate[:30]}... -> {cleaned_output[:30]}...") # å‡å°‘å†—ä½™æ‰“å°
            return cleaned_output
        else:
            # print(f"      è­¦å‘Š: è®¯é£æ˜Ÿç« API ç¿»è¯‘è¿”å›ä¸ºç©ºæˆ–ä¸åŸæ–‡ç›¸åŒã€‚") # å‡å°‘å†—ä½™æ‰“å°
            return text # è¿”å›åŸæ–‡
    except requests.exceptions.RequestException as req_e:
        print(f"      è°ƒç”¨è®¯é£æ˜Ÿç« API ç¿»è¯‘æ—¶å‘ç”Ÿç½‘ç»œæˆ–HTTPé”™è¯¯: {req_e}")
        if 'response' in locals() and response is not None:
            print(f"      å“åº”çŠ¶æ€ç : {response.status_code}")
            try: print(f"      å“åº”å†…å®¹: {response.json()}")
            except json.JSONDecodeError: print(f"      å“åº”å†…å®¹ (éJSON): {response.text}")
        return text
    except Exception as e:
        print(f"      è°ƒç”¨è®¯é£æ˜Ÿç« API ç¿»è¯‘æ—¶å‡ºé”™: {e}")
        return text

# --- (A) RSS æºè·å–å‡½æ•° ---
def fetch_articles_from_rss(rss_url, source_name_override=None):
    print(f"    æ­£åœ¨ä» RSS æºè·å–: {rss_url} ({source_name_override or 'æœªçŸ¥æº'})")
    articles = []
    try:
        # ä½¿ç”¨æ›´é€šç”¨çš„ User-Agent
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'}
        response = requests.get(rss_url, headers=headers, timeout=25, allow_redirects=True) # å¢åŠ è¶…æ—¶æ—¶é—´

        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é”™è¯¯æˆ–éªŒè¯é¡µé¢
        if response.url != rss_url and ("apology" in response.url or "abuse" in response.url or "error" in response.url):
            print(f"      è¯·æ±‚è¢«é‡å®šå‘åˆ°ç–‘ä¼¼é”™è¯¯/éªŒè¯é¡µé¢: {response.url}")
            # å°è¯•è¯»å–å†…å®¹åˆ¤æ–­æ˜¯å¦çœŸçš„å¤±è´¥
            if response.status_code >= 400:
                 print(f"      é‡å®šå‘åé¡µé¢çŠ¶æ€ç ä¸º {response.status_code}ï¼Œåˆ¤å®šä¸ºå¤±è´¥ã€‚")
                 response.raise_for_status() # æŠ›å‡º HTTPError
            else:
                 print(f"      é‡å®šå‘åé¡µé¢çŠ¶æ€ç ä¸º {response.status_code}ï¼Œå°è¯•ç»§ç»­è§£æã€‚")
        else:
             response.raise_for_status() # æ£€æŸ¥åŸå§‹è¯·æ±‚çš„çŠ¶æ€ç 

        feed = feedparser.parse(response.content)
        if feed.bozo:
             print(f"      è­¦å‘Š: feedparser è§£æ RSS æºæ—¶é‡åˆ°é—®é¢˜ (å¯èƒ½ä¸ä¸¥é‡): {feed.bozo_exception} (URL: {rss_url})")
        if not feed.entries:
            print(f"      æ­¤ RSS æºæœªè¿”å›ä»»ä½•æ¡ç›®: {rss_url}")
            return []

        print(f"      ä»æ­¤ RSS æºåŸå§‹è·å–åˆ° {len(feed.entries)} æ¡æ–°é—»ã€‚")
        for entry in feed.entries:
            title = entry.get("title", "æ— æ ‡é¢˜")
            link = entry.get("link", f"javascript:void(0);_{html.escape(title)}")

            # å°è¯•è·å–å‘å¸ƒæ—¥æœŸ (feedparser ä¼šå°è¯•è§£æå¤šç§æ ¼å¼)
            published_time_struct = entry.get("published_parsed") or entry.get("updated_parsed")
            # å¦‚æœ feedparser æ²¡è§£æå‡ºæ¥ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæå–å¹¶ç”¨ dateutil è§£æ
            if not published_time_struct:
                date_str = entry.get("published") or entry.get("updated") or entry.get("dc_date") # å°è¯•æ›´å¤šå¯èƒ½çš„æ—¥æœŸå­—æ®µ
                if date_str:
                    published_time_struct = parse_date_flexible(date_str)
                    # if published_time_struct:
                    #      print(f"      ä½¿ç”¨ dateutil æˆåŠŸè§£ææ—¥æœŸ: {date_str}") # å‡å°‘å†—ä½™æ‰“å°
                    # else:
                    #      print(f"      è­¦å‘Š: æœªèƒ½è§£ææ—¥æœŸå­—ç¬¦ä¸²: {date_str} for '{title[:30]}...'") # å‡å°‘å†—ä½™æ‰“å°

            summary_html = entry.get("summary", entry.get("description", "æš‚æ— æ‘˜è¦"))
            snippet = clean_html(summary_html)
            actual_source_name = source_name_override
            if not actual_source_name:
                source_info = entry.get("source")
                actual_source_name = source_info.get("title") if source_info else "æœªçŸ¥æ¥æº"
                # Google News ç‰¹æ®Šå¤„ç† (å¯èƒ½éœ€è¦è°ƒæ•´)
                if "news.google.com" in rss_url and not source_name_override:
                    if ' - ' in title:
                        parts = title.rsplit(' - ', 1)
                        if len(parts) == 2 and parts[1]: # ç¡®ä¿åˆ†å‰²æˆåŠŸä¸”ç¬¬äºŒéƒ¨åˆ†éç©º
                             actual_source_name = parts[1]
                             title = parts[0] # ä»æ ‡é¢˜ä¸­ç§»é™¤æ¥æº
            articles.append({
                "title": title.strip(), # æ¸…ç†æ ‡é¢˜ä¸¤ç«¯ç©ºæ ¼
                "url": link,
                "snippet": snippet,
                "source": actual_source_name,
                "time_struct": published_time_struct # ç›´æ¥å­˜å‚¨ time.struct_time å¯¹è±¡
            })
    except requests.exceptions.Timeout: print(f"      è·å– RSS æºæ—¶å‘ç”Ÿè¶…æ—¶é”™è¯¯: {rss_url}")
    except requests.exceptions.SSLError as ssl_e: print(f"      è·å– RSS æºæ—¶å‘ç”Ÿ SSL é”™è¯¯: {ssl_e} (URL: {rss_url})")
    except requests.exceptions.RequestException as e: print(f"      è·å– RSS æºæ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e} (URL: {rss_url})")
    except Exception as e: print(f"      å¤„ç† RSS æºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e} (URL: {rss_url})")
    return articles

# --- (B) çˆ¬è™«å‡½æ•°å®šä¹‰ ---

def fetch_breakthrought1d_articles():
    BASE_URL = "https://www.breakthrought1d.org/news/"
    print(f"    æ­£åœ¨çˆ¬å–: {BASE_URL}")
    articles = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(BASE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        # *** æ›´æ–°çš„é€‰æ‹©å™¨ (éœ€è¦æ ¹æ®å®é™…ç½‘ç«™ç»“æ„éªŒè¯å’Œè°ƒæ•´) ***
        for article_el in soup.select("article.post, div.news-item"): # å°è¯•å¸¸è§çš„æ–‡ç« å®¹å™¨é€‰æ‹©å™¨
            a_tag = article_el.select_one("h2 a, h3 a, .entry-title a, .news-title a") # å°è¯•å¸¸è§çš„æ ‡é¢˜é“¾æ¥é€‰æ‹©å™¨
            if not a_tag: continue
            title_en = a_tag.get_text(strip=True)
            link = urljoin(BASE_URL, a_tag.get("href"))
            summary_tag = article_el.select_one("div.entry-summary p, div.post-excerpt p, .news-summary p") # å°è¯•å¸¸è§çš„æ‘˜è¦é€‰æ‹©å™¨
            summary_en = summary_tag.get_text(strip=True) if summary_tag else ""

            time_struct = None
            date_tag = article_el.select_one("time.published, span.posted-on time, .post-date, .news-date") # å°è¯•å¸¸è§çš„æ—¥æœŸé€‰æ‹©å™¨
            if date_tag:
                date_str = date_tag.get_text(strip=True) or date_tag.get('datetime')
                if date_str:
                    time_struct = parse_date_flexible(date_str) # ä½¿ç”¨çµæ´»çš„æ—¥æœŸè§£æ

            # if not time_struct: print(f"      è­¦å‘Š: æœªèƒ½ä» {link} æå–å‘å¸ƒæ—¥æœŸã€‚") # å‡å°‘å†—ä½™æ‰“å°

            articles.append({
                "title": title_en, "url": link, "snippet": summary_en,
                "source": "Breakthrough T1D", "time_struct": time_struct,
                "needs_translation": True
            })
    except Exception as e: print(f"      çˆ¬å– Breakthrough T1D æ—¶å‡ºé”™: {e}")
    return articles

def fetch_dzd_articles():
    BASE_URL = "https://www.dzd-ev.de/en/press/press-releases/press-releases-2025/index.html" # æ³¨æ„å¹´ä»½å¯èƒ½éœ€è¦æ›´æ–°
    print(f"    æ­£åœ¨çˆ¬å–: {BASE_URL}")
    articles = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(BASE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # *** æ›´æ–°çš„é€‰æ‹©å™¨ (éœ€è¦æ ¹æ®å®é™…ç½‘ç«™ç»“æ„éªŒè¯å’Œè°ƒæ•´) ***
        for item in soup.select("div.news-list-item, div.teaser"): # å°è¯•æ›´æ–°çš„é€‰æ‹©å™¨
            title_tag = item.select_one("h3 a, h2 a, .news-title a")
            if not title_tag: continue
            title_en = title_tag.get_text(strip=True)
            link = urljoin(BASE_URL, title_tag.get("href"))

            summary_tag = item.select_one("p, .teaser-text p, .news-teaser p")
            summary_en = summary_tag.get_text(strip=True) if summary_tag else ""

            time_struct = None
            date_span = item.select_one("span.date, div.date, .news-date") # å°è¯•æ›´æ–°çš„æ—¥æœŸé€‰æ‹©å™¨
            if date_span:
                date_str = date_span.get_text(strip=True)
                time_struct = parse_date_flexible(date_str) # ä½¿ç”¨çµæ´»çš„æ—¥æœŸè§£æ

            # if not time_struct: print(f"      è­¦å‘Š: æœªèƒ½ä» {link} æå–å‘å¸ƒæ—¥æœŸã€‚") # å‡å°‘å†—ä½™æ‰“å°

            articles.append({
                "title": title_en, "url": link, "snippet": summary_en,
                "source": "DZD News", "time_struct": time_struct,
                "needs_translation": True
            })
    except Exception as e: print(f"      çˆ¬å– DZD News æ—¶å‡ºé”™: {e}")
    return articles

def fetch_pubmed_articles():
    if not Entrez:
        print("    é”™è¯¯: Biopython (Entrez) æœªåŠ è½½ï¼Œè·³è¿‡ PubMed API è°ƒç”¨ã€‚")
        return []
    Entrez.email = os.getenv("PUBMED_API_EMAIL", "default_email@example.com")
    search_term = "(diabetes[Title/Abstract]) AND (treatment[Title/Abstract] OR research[Title/Abstract] OR prevention[Title/Abstract])"
    print(f"    æ­£åœ¨é€šè¿‡ PubMed API æœç´¢: {search_term}")
    articles = []
    MAX_PUBMED_RESULTS = 20
    try:
        handle_search = Entrez.esearch(db="pubmed", term=search_term, retmax=MAX_PUBMED_RESULTS, sort="pub date")
        record_search = Entrez.read(handle_search)
        handle_search.close()
        id_list = record_search["IdList"]
        if not id_list:
            print("      PubMed API æœªè¿”å›ä»»ä½•æ–‡ç«  IDã€‚")
            return []
        
        print(f"      PubMed API è¿”å› {len(id_list)} ä¸ªæ–‡ç«  IDï¼Œæ­£åœ¨è·å–æ‘˜è¦...")
        handle_summary = Entrez.esummary(db="pubmed", id=",".join(id_list))
        summary_record = Entrez.read(handle_summary)
        handle_summary.close()

        for docsum in summary_record:
            title_en = docsum.get("Title", "No Title Available")
            pmid = docsum.get("Id", "")
            link = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "javascript:void(0);"
            journal_source = docsum.get("Source", "")
            authors = docsum.get("AuthorList", [])
            authors_str = ", ".join(authors[:3]) + (" et al." if len(authors) > 3 else "") if authors else "N/A"
            snippet_en = f"Journal: {journal_source}. Authors: {authors_str}." if journal_source or authors else "No Summary Available"

            pubdate_str = docsum.get("PubDate", "")
            time_struct = None
            if pubdate_str:
                time_struct = parse_date_flexible(pubdate_str) # ä½¿ç”¨çµæ´»çš„æ—¥æœŸè§£æ
                # if not time_struct:
                #     print(f"      è­¦å‘Š: æœªèƒ½è§£æ PubMed æ—¥æœŸå­—ç¬¦ä¸²: {pubdate_str} for PMID {pmid}") # å‡å°‘å†—ä½™æ‰“å°

            # å³ä½¿æ—¥æœŸè§£æå¤±è´¥ï¼Œä¹Ÿæ·»åŠ æ–‡ç« ï¼Œæ—¥æœŸè¿‡æ»¤åœ¨ä¸»å¾ªç¯ä¸­è¿›è¡Œ
            articles.append({
                "title": title_en, "url": link, "snippet": snippet_en,
                "source": "PubMed", "time_struct": time_struct,
                "needs_translation": True
            })
            time.sleep(0.4)
    except Exception as e: print(f"      å¤„ç† PubMed API æ—¶å‡ºé”™: {e}")
    return articles

def fetch_idf_articles():
    url = "https://www.idf.org/news"
    print(f"    æ­£åœ¨çˆ¬å–: {url}")
    articles = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # *** æ›´æ–°çš„é€‰æ‹©å™¨ (éœ€è¦æ ¹æ®å®é™…ç½‘ç«™ç»“æ„éªŒè¯å’Œè°ƒæ•´) ***
        for item in soup.select("article.news-item, div.news-item, li.news-list-item, .card"): # å°è¯•æ›´å¤šé€‰æ‹©å™¨
            title_tag = item.select_one("h3 a, h2 a, .title a, .news-title a, .card-title a")
            if not title_tag: continue
            
            title_en = title_tag.get_text(strip=True)
            link = urljoin(url, title_tag.get("href"))

            summary_tag = item.select_one("p, .summary, .excerpt, .news-excerpt, .card-text")
            summary_en = summary_tag.get_text(strip=True) if summary_tag else ""

            time_struct = None
            date_tag = item.select_one("time, .date, .post-date, .news-date, .card-date") # å°è¯•æ›´å¤šæ—¥æœŸé€‰æ‹©å™¨
            if date_tag:
                date_str = date_tag.get_text(strip=True) or date_tag.get('datetime')
                if date_str:
                     time_struct = parse_date_flexible(date_str) # ä½¿ç”¨çµæ´»çš„æ—¥æœŸè§£æ

            # if not time_struct: print(f"      è­¦å‘Š: æœªèƒ½ä» {link} æå–å‘å¸ƒæ—¥æœŸã€‚") # å‡å°‘å†—ä½™æ‰“å°
            
            articles.append({
                "title": title_en, "url": link, "snippet": summary_en,
                "source": "IDF News", "time_struct": time_struct,
                "needs_translation": True
            })
    except Exception as e: print(f"      çˆ¬å– IDF News æ—¶å‡ºé”™: {e}")
    return articles


SCRAPER_FUNCTIONS_MAP = {
    "fetch_breakthrought1d_articles": fetch_breakthrought1d_articles,
    # "fetch_myglu_articles": fetch_myglu_articles, # æ³¨é‡Šæ‰çš„ä¿æŒä¸å˜
    "fetch_dzd_articles": fetch_dzd_articles,
    # "fetch_adces_articles": fetch_adces_articles, # æ³¨é‡Šæ‰çš„ä¿æŒä¸å˜
    # "fetch_panther_articles": fetch_panther_articles, # æ³¨é‡Šæ‰çš„ä¿æŒä¸å˜
    # "fetch_nmpa_articles": fetch_nmpa_articles, # æ³¨é‡Šæ‰çš„ä¿æŒä¸å˜
    "fetch_pubmed_articles": fetch_pubmed_articles,
    "fetch_idf_articles": fetch_idf_articles,
}

# --- (C) ä½¿ç”¨è®¯é£æ˜Ÿç« HTTP API è¿›è¡ŒåŠ¨æ€åˆ†ç±» ---
def categorize_article_with_llm(article_obj):
    """ä½¿ç”¨è®¯é£æ˜Ÿç« HTTP API å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±» (using Bearer Token for SPARK_LITE_HTTP_URL)"""
    global llm_call_count, MAX_LLM_CALLS
    if not SPARK_API_PASSWORD:
        print("      é”™è¯¯: è®¯é£æ˜Ÿç« APIPassword æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œ LLM åˆ†ç±»ã€‚å°†å½’å…¥'ç»¼åˆèµ„è®¯'ã€‚")
        return "ç»¼åˆèµ„è®¯"
    if llm_call_count >= MAX_LLM_CALLS:
        # print(f"      è­¦å‘Š: å·²è¾¾åˆ° LLM è°ƒç”¨æ¬¡æ•°ä¸Šé™ ({MAX_LLM_CALLS})ï¼Œè·³è¿‡åˆ†ç±»ã€‚") # å‡å°‘å†—ä½™æ‰“å°
        return "ç»¼åˆèµ„è®¯"

    title = article_obj.get("title", "")
    snippet = article_obj.get("snippet", "")
    text_to_classify = f"æ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{snippet[:300]}"

    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹ï¼Œåˆ¤æ–­å®ƒæœ€ç¬¦åˆä¸‹åˆ—å“ªä¸ªåˆ†ç±»ï¼Ÿè¯·ä¸¥æ ¼ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªï¼Œå¹¶åªè¿”å›åˆ†ç±»åç§°ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è§£é‡Šæˆ–æ–‡å­—ã€‚

å¯é€‰åˆ†ç±»åˆ—è¡¨ï¼š{', '.join(VALID_CATEGORY_NAMES)}

æ–‡ç« å†…å®¹ï¼š
{text_to_classify}

æœ€åˆé€‚çš„åˆ†ç±»åç§°æ˜¯ï¼š"""

    # print(f"      æ­£åœ¨è°ƒç”¨è®¯é£æ˜Ÿç« HTTP API å¯¹ '{title[:30]}...' è¿›è¡Œåˆ†ç±»...") # å‡å°‘å†—ä½™æ‰“å°
    llm_call_count += 1

    try:
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {SPARK_API_PASSWORD}"}
        payload = {
            "model": "lite",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 50
        }
        response = requests.post(SPARK_LITE_HTTP_URL, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        response_data = response.json()

        llm_output = ""
        if 'choices' in response_data and response_data['choices']:
            message = response_data['choices'][0].get('message', {})
            llm_output = message.get('content', '').strip()
        elif 'header' in response_data and response_data['header'].get('code') != 0:
            print(f"      è®¯é£æ˜Ÿç« API è¿”å›é”™è¯¯: code={response_data['header'].get('code')}, message={response_data['header'].get('message')}")
            return "ç»¼åˆèµ„è®¯"
        else:
            print(f"      è­¦å‘Š: æœªçŸ¥çš„è®¯é£æ˜Ÿç« API åˆ†ç±»å“åº”ç»“æ„: {response_data}")
            return "ç»¼åˆèµ„è®¯"

        # print(f"      è®¯é£æ˜Ÿç« API è¿”å›: '{llm_output}'") # å‡å°‘å†—ä½™æ‰“å°

        cleaned_output = llm_output.strip().strip('"').strip("'").replace("ï¼š","").replace(":","")
        if cleaned_output in VALID_CATEGORY_NAMES:
            # print(f"      æ–‡ç«  '{title[:30]}...' æˆåŠŸåˆ†ç±»åˆ° '{cleaned_output}'") # å‡å°‘å†—ä½™æ‰“å°
            return cleaned_output
        else:
            for valid_cat in VALID_CATEGORY_NAMES:
                if valid_cat in cleaned_output:
                    print(f"      è­¦å‘Š: LLM è¿”å›çš„åˆ†ç±» '{cleaned_output}' (åŸå§‹: '{llm_output}') åŒ…å«æœ‰æ•ˆåˆ†ç±» '{valid_cat}'. ä½¿ç”¨ '{valid_cat}'.")
                    return valid_cat
            print(f"      è­¦å‘Š: è®¯é£æ˜Ÿç« API è¿”å›çš„åˆ†ç±» '{cleaned_output}' (åŸå§‹: '{llm_output}') æ— æ•ˆæˆ–ä¸åœ¨åˆ—è¡¨ä¸­ã€‚å°†å½’å…¥'ç»¼åˆèµ„è®¯'ã€‚")
            return "ç»¼åˆèµ„è®¯"

    except requests.exceptions.RequestException as req_e:
        print(f"      è°ƒç”¨è®¯é£æ˜Ÿç« API åˆ†ç±»æ—¶å‘ç”Ÿç½‘ç»œæˆ–HTTPé”™è¯¯: {req_e}")
        if 'response' in locals() and response is not None:
            print(f"      å“åº”çŠ¶æ€ç : {response.status_code}")
            try: print(f"      å“åº”å†…å®¹: {response.json()}")
            except json.JSONDecodeError: print(f"      å“åº”å†…å®¹ (éJSON): {response.text}")
        return "ç»¼åˆèµ„è®¯"
    except Exception as e:
        print(f"      è°ƒç”¨è®¯é£æ˜Ÿç« API åˆ†ç±»æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "ç»¼åˆèµ„è®¯"

# --- HTML ç”Ÿæˆé€»è¾‘ ---
def generate_html_content(all_news_data_sorted):
    current_time_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
    app_timezone = os.getenv('APP_TIMEZONE', 'UTC')
    if app_timezone != 'UTC':
        try:
            tz_offset = int(os.getenv('APP_TIMEZONE_OFFSET_HOURS', '8'))
            current_time_obj = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=tz_offset)))
            current_time_str = current_time_obj.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S %Z')
        except Exception as e:
            print(f"Error applying timezone: {e}. Falling back to server time.")
            current_time_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (æœåŠ¡å™¨æ—¶é—´)')

    current_year = datetime.datetime.now().year
    github_repo_url = f"https://github.com/{os.getenv('GITHUB_REPOSITORY', 'doudou-ux/diabetes-news')}"
    html_output = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç³–å°¿ç—…å‰æ²¿èµ„è®¯</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {{ font-family: 'Inter', sans-serif; background-color: #f0f4f8; }}
        .news-card {{ transition: transform 0.3s ease, box-shadow 0.3s ease; display: flex; flex-direction: column; justify-content: space-between; height: 100%; background-color: #ffffff; border-radius: 0.5rem; }}
        .news-card:hover {{ transform: translateY(-5px); box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); }}
        .tab-content {{ background-color: #ffffff; border-radius: 0.75rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); padding: 1.25rem; display: none; }}
        .tab-content.active {{ display: block; }}
        @media (min-width: 768px) {{ .tab-content {{ padding: 2rem; }} }}
        .category-title-text {{ border-bottom: 3px solid #3b82f6; padding-bottom: 0.75rem; margin-bottom: 1.75rem; color: #1e40af; font-size: 1.5rem; }}
        @media (min-width: 768px) {{ .category-title-text {{ font-size: 1.875rem; }} }}
        .news-item-link {{ color: #2563eb; text-decoration: none; font-weight: 600; }}
        .news-item-link:hover {{ text-decoration: underline; color: #1d4ed8; }}
        .source-tag, .time-tag {{ padding: 0.25rem 0.75rem; border-radius: 0.375rem; font-size: 0.75rem; font-weight: 500; line-height: 1.25; }}
        .source-tag {{ background-color: #e0e7ff; color: #3730a3; }}
        .time-tag {{ background-color: #d1fae5; color: #065f46; }}
        .card-content {{ flex-grow: 1; }}
        .card-footer {{ margin-top: auto; }} /* Pushes footer to bottom */
        .header-main-title {{ font-size: 1.875rem; }} /* 30px */
        @media (min-width: 640px) {{ .header-main-title {{ font-size: 2.25rem; }} }} /* 36px */
        @media (min-width: 768px) {{ .header-main-title {{ font-size: 3rem; }} }} /* 48px */
        .main-container {{ padding: 1rem; }}
        @media (min-width: 768px) {{ .main-container {{ padding: 2rem; }} }}
        .loader {{ border: 5px solid #f3f3f3; border-radius: 50%; border-top: 5px solid #3498db; width: 50px; height: 50px; -webkit-animation: spin 1s linear infinite; animation: spin 1s linear infinite; margin: 20px auto; }}
        @-webkit-keyframes spin {{ 0% {{ -webkit-transform: rotate(0deg); }} 100% {{ -webkit-transform: rotate(360deg); }} }}
        @keyframes spin {{ 0% {{ transform: rotate(0deg); }} 100% {{ transform: rotate(360deg); }} }}
        .tab-buttons-container {{ display: flex; flex-wrap: wrap; margin-bottom: 1.5rem; border-bottom: 2px solid #d1d5db; }}
        .tab-button {{ padding: 0.75rem 1.25rem; margin-right: 0.5rem; margin-bottom: -2px; /* Overlap border */ border: 2px solid transparent; border-bottom: none; border-top-left-radius: 0.375rem; border-top-right-radius: 0.375rem; cursor: pointer; font-weight: 500; color: #4b5563; background-color: #f9fafb; transition: all 0.3s ease; }}
        .tab-button:hover {{ color: #1f2937; background-color: #f3f4f6; }}
        .tab-button.active {{ color: #1d4ed8; background-color: #ffffff; border-color: #d1d5db; border-bottom-color: #ffffff; /* Make bottom border of active tab white to blend with content */ }}
    </style>
</head>
<body class="bg-gray-100 text-gray-800">
    <div class="container mx-auto main-container max-w-screen-xl"> 
        <header class="text-center mb-10 md:mb-16">
            <h1 class="font-bold text-blue-700 header-main-title">ç³–å°¿ç—…å‰æ²¿èµ„è®¯</h1>
            <p class="text-gray-600 mt-3 text-base md:text-lg">æœ€è¿‘ä¸€ä¸ªæœˆåŠ¨æ€ï¼ˆè‡ªåŠ¨æ›´æ–°äºï¼š<span id="updateTime">{current_time_str}</span>ï¼‰</p>
            <p class="text-sm text-gray-500 mt-2">èµ„è®¯ç»¼åˆæ¥æº (ç”± AI æ™ºèƒ½åˆ†ç±»)</p>
        </header>
        <div class="tab-buttons-container" id="tabButtons">"""
    first_active_category_key = None
    if any(all_news_data_sorted.values()):
        for cat_key in CATEGORIES_CONFIG.keys():
            if all_news_data_sorted.get(cat_key):
                first_active_category_key = cat_key
                break
    if not first_active_category_key:
         first_active_category_key = list(CATEGORIES_CONFIG.keys())[0] if CATEGORIES_CONFIG else None

    for category_name_key in CATEGORIES_CONFIG.keys():
        category_config = CATEGORIES_CONFIG.get(category_name_key, {})
        emoji = category_config.get("emoji", "")
        tab_id = "tab-" + html.escape(category_name_key.replace(" ", "-").replace("/", "-").lower())
        is_active = (category_name_key == first_active_category_key)
        active_class = "active" if is_active else ""
        html_output += f"""<button class="tab-button {active_class}" data-tab-target="#{tab_id}">{emoji} {html.escape(category_name_key)}</button>"""

    html_output += """</div><div id="news-content">"""
    if not any(all_news_data_sorted.values()):
        html_output += '<p class="text-center text-gray-500 text-xl py-10">æŠ±æ­‰ï¼Œç›®å‰æœªèƒ½åŠ è½½åˆ°æœ€è¿‘ä¸€ä¸ªæœˆç›¸å…³çš„ç³–å°¿ç—…èµ„è®¯ã€‚</p>'
    else:
        for category_name_key in CATEGORIES_CONFIG.keys():
            articles = all_news_data_sorted.get(category_name_key, []) 
            category_config = CATEGORIES_CONFIG.get(category_name_key, {})
            emoji = category_config.get("emoji", "")
            tab_id = "tab-" + html.escape(category_name_key.replace(" ", "-").replace("/", "-").lower())
            is_active_content = (category_name_key == first_active_category_key)
            active_class = "active" if is_active_content else ""
            
            category_html_content = f"""<div id="{tab_id}" class="tab-content {active_class}"><h2 class="font-semibold category-title-text">{emoji} {html.escape(category_name_key)}</h2>"""
            if not articles:
                category_html_content += '<p class="text-gray-500">æœ€è¿‘ä¸€ä¸ªæœˆæš‚æ— è¯¥åˆ†ç±»ä¸‹çš„èµ„è®¯ã€‚</p>'
            else:
                category_html_content += '<div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6 md:gap-8 news-grid">'
                for article in articles: 
                    title = html.escape(article.get('title', 'æ— æ ‡é¢˜'))
                    url = html.escape(article.get('url', 'javascript:void(0);'))
                    snippet_raw = article.get('snippet', 'æš‚æ— æ‘˜è¦')
                    snippet = html.escape(snippet_raw[:150] + ('...' if len(snippet_raw) > 150 else ''))
                    source_display = html.escape(article.get('source', 'æœªçŸ¥æ¥æº'))
                    time_display = html.escape(article.get('time_display_str', 'æœªçŸ¥æ—¶é—´'))
                    
                    category_html_content += f"""
                    <div class="news-card shadow-md hover:shadow-lg p-5">
                        <div class="card-content">
                            <h3 class="text-lg mb-2 font-semibold">
                                <a href="{url}" {'target="_blank" rel="noopener noreferrer"' if url != 'javascript:void(0);' and url.startswith('http') else ''} class="news-item-link">{title}</a>
                            </h3>
                            <p class="text-gray-600 text-sm mb-4 leading-relaxed">{snippet}</p>
                        </div>
                        <div class="card-footer text-xs text-gray-500 flex flex-wrap gap-2 items-center pt-3 border-t border-gray-200">
                            <span class="source-tag">{source_display}</span>
                            <span class="time-tag">{time_display}</span>
                        </div>
                    </div>"""
                category_html_content += "</div>" 
            category_html_content += "</div>"
            html_output += category_html_content

    html_output += f"""
        </div> </div> <footer class="text-center p-6 mt-12 text-gray-600 text-sm border-t border-gray-300"><p>&copy; {current_year} ç³–å°¿ç—…èµ„è®¯èšåˆ. <a href="{github_repo_url}" target="_blank" rel="noopener noreferrer" class="text-blue-600 hover:underline">é¡¹ç›®æºç </a></p><p class="mt-1">æœ¬ç«™å†…å®¹ä»…ä¾›å‚è€ƒ, ä¸æ„æˆåŒ»ç–—å»ºè®®ã€‚</p></footer>
    <script>
        document.addEventListener('DOMContentLoaded', function () {{
            const tabButtons = document.querySelectorAll('.tab-button');
            const tabContents = document.querySelectorAll('.tab-content');
            
            let activeTabButton = document.querySelector('.tab-button.active');
            let activeTabContent = document.querySelector('.tab-content.active');

            if (!activeTabButton && tabButtons.length > 0) {{
                tabButtons[0].classList.add('active');
                activeTabButton = tabButtons[0];
            }}
            if (activeTabButton && !activeTabContent) {{
                const targetId = activeTabButton.dataset.tabTarget;
                const targetContent = document.querySelector(targetId);
                if (targetContent) {{
                    targetContent.classList.add('active');
                }} else if (tabContents.length > 0) {{
                    tabContents[0].classList.add('active');
                }}
            }} else if (activeTabButton && activeTabContent) {{
                if (activeTabButton.dataset.tabTarget !== '#' + activeTabContent.id) {{
                    tabContents.forEach(content => content.classList.remove('active'));
                    const targetContent = document.querySelector(activeTabButton.dataset.tabTarget);
                    if (targetContent) targetContent.classList.add('active');
                }}
            }}

            tabButtons.forEach(button => {{
                button.addEventListener('click', () => {{
                    tabButtons.forEach(btn => btn.classList.remove('active'));
                    tabContents.forEach(content => content.classList.remove('active'));
                    
                    button.classList.add('active');
                    const targetContentId = button.dataset.tabTarget;
                    const targetContent = document.querySelector(targetContentId);
                    if (targetContent) {{ targetContent.classList.add('active'); }}
                }});
            }});
        }});
    </script>
</body></html>"""
    return html_output

# --- (5) ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == "__main__":
    print("å¼€å§‹ä»å¤šä¸ª RSS æºå’Œçˆ¬è™«ç”Ÿæˆç³–å°¿ç—…èµ„è®¯ç½‘é¡µ...")
    
    unique_articles_candidates = {}
    globally_seen_urls = set()
    today = datetime.date.today()
    MAX_ARTICLES_PER_CATEGORY = 10
    MAX_LLM_CALLS = int(os.getenv("MAX_LLM_CALLS", "500"))
    llm_call_count = 0

    # --- æ­¥éª¤ä¸€ï¼šä»æƒå¨ RSS æºè·å–æ–°é—» ---
    print("\n--- æ­£åœ¨ä»æƒå¨ RSS æºè·å–æ–°é—» ---")
    for feed_info in AUTHORITATIVE_RSS_FEEDS:
        print(f"  å¤„ç† RSS æº: {feed_info['source_override']}")
        current_priority = feed_info.get("priority", 5) 
        needs_translation_feed = feed_info.get("needs_translation", False)
        raw_articles_from_feed = fetch_articles_from_rss(feed_info["url"], feed_info["source_override"])
        
        processed_in_feed = 0
        skipped_due_date = 0
        added_to_candidates = 0

        for article_data in raw_articles_from_feed:
            if article_data["url"] in globally_seen_urls: continue
            
            title_to_process = article_data["title"]
            snippet_to_process = article_data["snippet"]
            time_struct = article_data["time_struct"] # è·å– time_struct

            # ä»…åœ¨éœ€è¦ç¿»è¯‘ä¸” LLM è°ƒç”¨æ¬¡æ•°æœªè¾¾ä¸Šé™æ—¶è¿›è¡Œç¿»è¯‘
            if needs_translation_feed and llm_call_count < MAX_LLM_CALLS:
                original_title_for_norm = title_to_process
                translated_title = translate_text_with_llm(title_to_process)
                if translated_title != title_to_process:
                    title_to_process = translated_title
                # else: print(f"      ç¿»è¯‘æ ‡é¢˜å¤±è´¥æˆ–è¿”å›åŸæ–‡: {title_to_process[:30]}...") # å‡å°‘å†—ä½™

                translated_snippet = translate_text_with_llm(snippet_to_process)
                if translated_snippet != snippet_to_process:
                     snippet_to_process = translated_snippet
                # else: print(f"      ç¿»è¯‘æ‘˜è¦å¤±è´¥æˆ–è¿”å›åŸæ–‡: {snippet_to_process[:30]}...") # å‡å°‘å†—ä½™
                # åªæœ‰åœ¨æˆåŠŸè°ƒç”¨ç¿»è¯‘APIåæ‰æš‚åœ
                if llm_call_count % 2 == 0: # æ¯ç¿»è¯‘ä¸¤é¡¹ï¼ˆæ ‡é¢˜å’Œæ‘˜è¦ï¼‰åæš‚åœä¸€æ¬¡
                    time.sleep(1.1)

            # *å…³é”®æ”¹åŠ¨*: æ£€æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆä¸”åœ¨æœ€è¿‘ä¸€ä¸ªæœˆå†…
            if time_struct and is_within_last_month(time_struct, today):
                normalized_title_key = normalize_title(article_data["title"])
                
                time_display_str = "æœªçŸ¥æ—¶é—´"
                try:
                    # ä½¿ç”¨è§£æå‡ºçš„ time_struct æ¥æ ¼å¼åŒ–æ—¥æœŸ
                    time_display_str = time.strftime("%Y-%m-%d", time_struct)
                except Exception:
                    pass
                
                article_obj_for_storage = {
                    "title": title_to_process, "url": article_data["url"], "snippet": snippet_to_process, 
                    "source": article_data["source"], "time_display_str": time_display_str, 
                    "time_struct": time_struct, # å­˜å‚¨è§£æåçš„ time_struct
                    "source_priority": current_priority, "source_type": "authoritative_rss"
                }
                
                if normalized_title_key not in unique_articles_candidates or \
                   current_priority > unique_articles_candidates[normalized_title_key]["priority"]:
                    unique_articles_candidates[normalized_title_key] = {
                        "article_obj": article_obj_for_storage, "priority": current_priority,
                        "url": article_data["url"]
                    }
                    globally_seen_urls.add(article_data["url"])
                    added_to_candidates += 1
            elif not time_struct:
                 # print(f"      è·³è¿‡æ–‡ç« ï¼ˆæ— æœ‰æ•ˆæ—¥æœŸï¼‰: {article_data['title'][:30]}...") # å‡å°‘å†—ä½™
                 skipped_due_date += 1
            else: # æ—¥æœŸæœ‰æ•ˆä½†ä¸åœ¨èŒƒå›´å†…
                 # print(f"      è·³è¿‡æ–‡ç« ï¼ˆæ—¥æœŸè¿‡æ—§ï¼‰: {article_data['title'][:30]}...") # å‡å°‘å†—ä½™
                 skipped_due_date += 1
            processed_in_feed += 1

        print(f"    æ¥è‡ª {feed_info['source_override']} å¤„ç†äº† {processed_in_feed} æ¡, æ–°å¢ {added_to_candidates} æ¡åˆ°å€™é€‰æ±  (å› æ—¥æœŸè·³è¿‡ {skipped_due_date} æ¡)ã€‚")
        time.sleep(1)

    # --- æ­¥éª¤äºŒï¼šä»çˆ¬è™«æºè·å–æ–°é—» ---
    print("\n--- æ­£åœ¨ä»çˆ¬è™«æºè·å–æ–°é—» ---")
    for scraper_info in SCRAPED_SOURCES_CONFIG:
        print(f"  å¤„ç†çˆ¬è™«æº: {scraper_info['source_override']}")
        if scraper_info["fetch_function"] not in SCRAPER_FUNCTIONS_MAP:
            print(f"    é”™è¯¯: æœªæ‰¾åˆ°çˆ¬è™«å‡½æ•° {scraper_info['fetch_function']}")
            continue
            
        fetch_function = SCRAPER_FUNCTIONS_MAP[scraper_info["fetch_function"]]
        current_priority = scraper_info.get("priority", 3)
        raw_articles_from_scraper = fetch_function()
        processed_in_scraper = 0
        skipped_due_date_scraper = 0
        added_to_candidates_scraper = 0

        for article_data in raw_articles_from_scraper: 
            if article_data["url"] in globally_seen_urls: continue

            title_to_process = article_data["title"]
            snippet_to_process = article_data["snippet"]
            needs_translation_scraper = article_data.get("needs_translation", False) 
            time_struct = article_data.get("time_struct") # è·å– time_struct

            if needs_translation_scraper and llm_call_count < MAX_LLM_CALLS:
                original_title_for_norm = title_to_process
                translated_title = translate_text_with_llm(title_to_process)
                if translated_title != title_to_process:
                    title_to_process = translated_title
                # else: print(f"      ç¿»è¯‘æ ‡é¢˜å¤±è´¥æˆ–è¿”å›åŸæ–‡: {title_to_process[:30]}...")

                translated_snippet = translate_text_with_llm(snippet_to_process)
                if translated_snippet != snippet_to_process:
                    snippet_to_process = translated_snippet
                # else: print(f"      ç¿»è¯‘æ‘˜è¦å¤±è´¥æˆ–è¿”å›åŸæ–‡: {snippet_to_process[:30]}...")
                if llm_call_count % 2 == 0:
                     time.sleep(1.1)
            
            if time_struct and is_within_last_month(time_struct, today):
                normalized_title_key = normalize_title(article_data["title"])
                
                time_display_str = "æœªçŸ¥æ—¶é—´"
                try:
                    time_display_str = time.strftime("%Y-%m-%d", time_struct)
                except Exception: pass
                
                article_obj_for_storage = {
                    "title": title_to_process, "url": article_data["url"], "snippet": snippet_to_process, 
                    "source": scraper_info["source_override"], "time_display_str": time_display_str, 
                    "time_struct": time_struct, 
                    "source_priority": current_priority, "source_type": "scraper"
                }
                if normalized_title_key not in unique_articles_candidates or \
                   current_priority > unique_articles_candidates[normalized_title_key]["priority"]:
                    unique_articles_candidates[normalized_title_key] = {
                        "article_obj": article_obj_for_storage, "priority": current_priority,
                        "url": article_data["url"]
                    }
                    globally_seen_urls.add(article_data["url"])
                    added_to_candidates_scraper += 1
            elif not time_struct:
                 # print(f"      è·³è¿‡æ–‡ç« ï¼ˆæ— æœ‰æ•ˆæ—¥æœŸï¼‰: {article_data['title'][:30]}...") # å‡å°‘å†—ä½™
                 skipped_due_date_scraper += 1
            else:
                 # print(f"      è·³è¿‡æ–‡ç« ï¼ˆæ—¥æœŸè¿‡æ—§ï¼‰: {article_data['title'][:30]}...") # å‡å°‘å†—ä½™
                 skipped_due_date_scraper += 1
            processed_in_scraper += 1

        print(f"    æ¥è‡ª {scraper_info['source_override']} å¤„ç†äº† {processed_in_scraper} æ¡, æ–°å¢ {added_to_candidates_scraper} æ¡åˆ°å€™é€‰æ±  (å› æ—¥æœŸè·³è¿‡ {skipped_due_date_scraper} æ¡)ã€‚")
        time.sleep(1)

    # --- æ­¥éª¤ä¸‰ï¼šä» Google News RSS è·å–è¡¥å……æ–°é—» ---
    print("\n--- æ­£åœ¨ä» Google News RSS è·å–è¡¥å……æ–°é—» (ç”¨äºå…¨å±€å€™é€‰æ± ) ---")
    google_search_term = "ç³–å°¿ç—… æ–°é—» OR diabetes news" 
    print(f"  ä½¿ç”¨ Google News æœç´¢è¯: {google_search_term}")
    google_news_rss_url = f"https://news.google.com/rss/search?q={quote_plus(google_search_term)}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
    
    raw_articles_from_google = fetch_articles_from_rss(google_news_rss_url, source_name_override=None)
    processed_in_google = 0
    skipped_due_date_google = 0
    added_to_candidates_google = 0

    for article_data in raw_articles_from_google:
        if article_data["url"] in globally_seen_urls: continue
            
        time_struct = article_data["time_struct"] # è·å– time_struct

        if time_struct and is_within_last_month(time_struct, today):
            normalized_title_key = normalize_title(article_data["title"])
            
            time_display_str = "æœªçŸ¥æ—¶é—´"
            try:
                time_display_str = time.strftime("%Y-%m-%d", time_struct)
            except Exception: pass
            
            title_to_process_g = article_data["title"]
            snippet_to_process_g = article_data["snippet"]
            # Google News é€šå¸¸ä¸éœ€è¦ç¿»è¯‘ (hl=zh-CN)ï¼Œå¦‚æœéœ€è¦ï¼Œåœ¨æ­¤å¤„æ·»åŠ ç¿»è¯‘é€»è¾‘

            article_obj_for_storage = {
                "title": title_to_process_g, "url": article_data["url"], "snippet": snippet_to_process_g, 
                "source": article_data["source"], "time_display_str": time_display_str, 
                "time_struct": time_struct, 
                "source_priority": GOOGLE_NEWS_PRIORITY, "source_type": "google_news"
            }

            if normalized_title_key not in unique_articles_candidates or \
               GOOGLE_NEWS_PRIORITY > unique_articles_candidates[normalized_title_key]["priority"]:
                unique_articles_candidates[normalized_title_key] = {
                    "article_obj": article_obj_for_storage, "priority": GOOGLE_NEWS_PRIORITY,
                    "url": article_data["url"]
                }
                globally_seen_urls.add(article_data["url"])
                added_to_candidates_google += 1
        elif not time_struct:
             skipped_due_date_google += 1
        else:
             skipped_due_date_google += 1
        processed_in_google += 1

    print(f"    æ¥è‡ª Google News å¤„ç†äº† {processed_in_google} æ¡, æ–°å¢ {added_to_candidates_google} æ¡åˆ°å€™é€‰æ±  (å› æ—¥æœŸæˆ–é‡å¤è·³è¿‡ {processed_in_google - added_to_candidates_google} æ¡)ã€‚")
    time.sleep(1)

    # --- æ­¥éª¤å››ï¼šä½¿ç”¨ LLM åŠ¨æ€åˆ†ç±»æ‰€æœ‰å€™é€‰æ–‡ç«  ---
    print(f"\n--- æ­£åœ¨å¯¹ {len(unique_articles_candidates)} æ¡å€™é€‰æ–‡ç« è¿›è¡ŒåŠ¨æ€åˆ†ç±» (LLM è°ƒç”¨ä¸Šé™: {MAX_LLM_CALLS}) ---")
    all_articles_by_site_category_temp = {category_name: [] for category_name in CATEGORIES_CONFIG.keys()}
    
    spark_categorization_ready = bool(SPARK_API_PASSWORD) 
    if not spark_categorization_ready:
        print("è­¦å‘Š: è®¯é£æ˜Ÿç« APIPassword æœªé…ç½®ï¼Œå°†è·³è¿‡ LLM åˆ†ç±»ï¼Œæ‰€æœ‰æ–‡ç« å½’å…¥'ç»¼åˆèµ„è®¯'ã€‚")
    
    categorized_count = 0
    llm_limit_hit_printed = False # æ ‡è®°æ˜¯å¦å·²æ‰“å° LLM ä¸Šé™è­¦å‘Š

    for candidate_key, candidate_info in unique_articles_candidates.items():
        article_to_categorize = candidate_info["article_obj"]

        best_category = "ç»¼åˆèµ„è®¯" 
        if spark_categorization_ready and llm_call_count < MAX_LLM_CALLS:
            try:
                best_category = categorize_article_with_llm(article_to_categorize)
                time.sleep(1.1) 
            except Exception as llm_e:
                print(f"    LLM åˆ†ç±»æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ for '{article_to_categorize['title'][:30]}...': {llm_e}ï¼Œæ–‡ç« å°†å½’å…¥'ç»¼åˆèµ„è®¯'ã€‚")
                best_category = "ç»¼åˆèµ„è®¯"
        elif llm_call_count >= MAX_LLM_CALLS and spark_categorization_ready and not llm_limit_hit_printed:
            print(f"    å·²è¾¾åˆ° LLM è°ƒç”¨æ¬¡æ•°ä¸Šé™ ({MAX_LLM_CALLS})ï¼Œå‰©ä½™æ–‡ç« å°†å½’å…¥'ç»¼åˆèµ„è®¯'ã€‚")
            llm_limit_hit_printed = True # åªæ‰“å°ä¸€æ¬¡
            best_category = "ç»¼åˆèµ„è®¯"
        elif not spark_categorization_ready: # å¦‚æœ API æœªé…ç½®ï¼Œç›´æ¥å½’å…¥ç»¼åˆ
             best_category = "ç»¼åˆèµ„è®¯"
        
        if best_category not in all_articles_by_site_category_temp:
            print(f"    è­¦å‘Š: LLM è¿”å›çš„åˆ†ç±» '{best_category}' ä¸åœ¨é¢„è®¾åˆ†ç±»ä¸­ for article '{article_to_categorize['title'][:30]}...'. å½’å…¥ 'ç»¼åˆèµ„è®¯'")
            best_category = "ç»¼åˆèµ„è®¯"
            
        all_articles_by_site_category_temp[best_category].append(article_to_categorize)
        categorized_count += 1
        # if categorized_count % 10 == 0: # å‡å°‘æ‰“å°é¢‘ç‡
        #     print(f"    å·²åˆ†ç±» {categorized_count}/{len(unique_articles_candidates)} æ–‡ç« ...")

    print(f"--- åˆ†ç±»å®Œæˆï¼Œå…±å¤„ç† {categorized_count} ç¯‡æ–‡ç«  ---")


    # --- æ­¥éª¤äº”ï¼šå¯¹æ¯ä¸ªåˆ†ç±»çš„æ–‡ç« æŒ‰æ¥æºç±»å‹å’Œæ—¥æœŸæ’åºå¹¶æˆªå– ---
    print("\n--- æ­£åœ¨å¯¹å„åˆ†ç±»æ–°é—»è¿›è¡Œæ’åºå’Œæˆªå– ---")
    all_articles_by_site_category_final_sorted = {}
    total_final_articles = 0
    for category_name, articles_list in all_articles_by_site_category_temp.items():
        articles_list.sort(key=lambda x: (
            SOURCE_TYPE_ORDER.get(x.get("source_type", "unknown"), 99),
            # ä½¿ç”¨ time.mktime å¤„ç† time_structï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è§†ä¸ºæœ€æ—©
            -(time.mktime(x["time_struct"]) if x.get("time_struct") else -float('inf')),
            -x.get("source_priority", 0)
        ), reverse=False) # ç¡®ä¿ sort æ˜¯å‡åºï¼ˆå› ä¸º key è¿”å›çš„æ˜¯å…ƒç»„ï¼Œé»˜è®¤å‡åºæ¯”è¾ƒï¼‰
        
        all_articles_by_site_category_final_sorted[category_name] = articles_list[:MAX_ARTICLES_PER_CATEGORY]
        count_in_category = len(all_articles_by_site_category_final_sorted[category_name])
        total_final_articles += count_in_category
        print(f"  åˆ†ç±» '{category_name}' æ’åºå¹¶æˆªå–åæœ‰ {count_in_category} æ¡æ–°é—»ã€‚")

    print(f"--- æ’åºæˆªå–å®Œæˆï¼Œæœ€ç»ˆå±•ç¤º {total_final_articles} ç¯‡æ–‡ç«  ---")


    # --- (6) ç”Ÿæˆæœ€ç»ˆçš„HTML ---
    print("\n--- æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„ HTML æ–‡ä»¶ ---")
    final_html = generate_html_content(all_articles_by_site_category_final_sorted)
    
    output_filename = "index.html" 
    try:
        with open(output_filename, "w", encoding="utf-8") as f:
            f.write(final_html)
        print(f"\næˆåŠŸç”Ÿæˆç½‘é¡µï¼š{output_filename}")
    except IOError as e:
        print(f"\né”™è¯¯ï¼šæ— æ³•å†™å…¥æ–‡ä»¶ {output_filename}ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    except Exception as e:
        print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    print(f"æ€»å…±è°ƒç”¨ LLM API {llm_call_count} æ¬¡ã€‚")
    print("èµ„è®¯ç½‘é¡µç”Ÿæˆå®Œæ¯•ã€‚")
